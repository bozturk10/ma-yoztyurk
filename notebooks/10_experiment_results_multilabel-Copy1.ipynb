{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src\n",
    "import numpy as np\n",
    "from src.paths import MODELS_DIR\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from src.data.process_data import process_open_ended, process_wave_data,process_open_ended_new\n",
    "from src.data.read_data import load_raw_survey_data, read_stata_file\n",
    "from src.paths import CODING_DIR, GLES_DIR, PROCESSED_DATA_DIR, ANNOTATED_GENERATIONS_DIR,RAW_DATA_DIR\n",
    "from src.utils import get_lang\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/12/1VAR_age\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9306/9306 [00:00<00:00, 10087.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/12/1VAR_berufabschluss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9306/9306 [00:01<00:00, 7075.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/12/1VAR_eastwest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9306/9306 [00:00<00:00, 11918.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/12/1VAR_gender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9306/9306 [00:00<00:00, 13191.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/12/1VAR_party\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9306/9306 [00:00<00:00, 38976.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/12/1VAR_schulabschluss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9306/9306 [00:01<00:00, 8694.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/12/Llama2_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9306/9306 [00:01<00:00, 6628.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/12/Llama2_base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9306/9306 [00:01<00:00, 8274.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/12/Llama2_model_opinion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9306/9306 [00:00<00:00, 10354.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/12/without_age\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9306/9306 [00:00<00:00, 45341.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/12/without_berufabschluss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9306/9306 [00:00<00:00, 13119.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/12/without_eastwest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9306/9306 [00:01<00:00, 6009.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/12/without_gender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9306/9306 [00:00<00:00, 9782.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/12/without_party\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9306/9306 [00:00<00:00, 16363.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/12/without_schulabschluss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9306/9306 [00:01<00:00, 6724.98it/s] \n"
     ]
    }
   ],
   "source": [
    "from src.analysis.ablationExperiment.utils import get_ablationExperiment_data\n",
    "(\n",
    "survey_labels_dict,\n",
    "llm_labels_dict,\n",
    "# multilabel\n",
    "survey_population_df_multilabel,  # df\n",
    "llm_population_df_multilabel,  # df\n",
    "survey_group_pmf_multilabel,  # dict of dfs\n",
    "llm_group_pmf_multilabel,  # dict of dfs\n",
    "# multiclass\n",
    "survey_population_df_multiclass,  # df\n",
    "llm_population_df_multiclass,  # df\n",
    "survey_group_pmf_multiclass,  # dict of dfs\n",
    "llm_group_pmf_multiclass,  # dict of dfs\n",
    ")=get_ablationExperiment_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_JS_waveExperiment' from 'src.analysis.waveExperiment.utils' (/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/src/analysis/waveExperiment/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwaveExperiment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_JS_waveExperiment\n\u001b[1;32m      4\u001b[0m population_JS, group_JS\u001b[38;5;241m=\u001b[39m get_JS_waveExperiment(survey_population_df, llm_population_df, survey_group_pmf, llm_group_pmf)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_JS_waveExperiment' from 'src.analysis.waveExperiment.utils' (/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/src/analysis/waveExperiment/utils.py)"
     ]
    }
   ],
   "source": [
    "from src.analysis.waveExperiment.utils import get_JS_waveExperiment\n",
    "\n",
    "\n",
    "population_JS, group_JS= get_JS_waveExperiment(survey_population_df, llm_population_df, survey_group_pmf, llm_group_pmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #compare ablation JS distances by group\n",
    "# mapped_dict = {\n",
    "#     \"ostwest\": [\"Llama2_base\", \"1VAR_eastwest\", \"without_eastwest\", \"Llama2_all\"],\n",
    "#     \"berufabschluss_clause\": [\n",
    "#         \"Llama2_base\",\n",
    "#         \"1VAR_berufabschluss\",\n",
    "#         \"without_berufabschluss\",\n",
    "#         \"Llama2_all\",\n",
    "#     ],\n",
    "#     \"leaning_party\": [\n",
    "#         \"Llama2_base\",\n",
    "#         \"1VAR_party\",\n",
    "#         \"without_party\",\n",
    "#         \"Llama2_all\",\n",
    "#     ],\n",
    "#     \"gender\": [\"Llama2_base\", \"1VAR_gender\", \"without_gender\", \"Llama2_all\"],\n",
    "#     \"schulabschluss_clause\": [\n",
    "#         \"Llama2_base\",\n",
    "#         \"1VAR_schulabschluss\",\n",
    "#         \"without_schulabschluss\",\n",
    "#         \"Llama2_all\",\n",
    "#     ],\n",
    "#     \"age_groups\": [\n",
    "#         \"Llama2_base\",\n",
    "#         \"1VAR_age\",\n",
    "#         \"without_age\",\n",
    "#         \"Llama2_all\",\n",
    "#     ],\n",
    "# }\n",
    "# a_list=[]\n",
    "# for category,experiments in mapped_dict.items():\n",
    "#     a=group_JS[group_JS['social_group_category'].isin([category]) &group_JS['wave'].isin(experiments)]\n",
    "#     sorted_experiments = {elt:i for i, elt in enumerate(experiments)}\n",
    "#     #print(category,':',sorted_experiments)\n",
    "#     a['wave_idx']=a['wave'].map(sorted_experiments)\n",
    "#     a=a.sort_values(by=['wave'], key=lambda x: x.map(sorted_experiments) ).reset_index()\n",
    "#     a_list.append(a)\n",
    "\n",
    "# ablation_js_df_filtered= pd.concat(a_list,axis=0)\n",
    "# ablation_js_df_filtered\n",
    "# #get_JS_group_plot_waveExperiment2(ablation_js_df_filtered,fname='ablation_JS_by_groups.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ablation_JS_plot(population_JS):\n",
    "    def filter_index(row):\n",
    "        a=row['index'][0]\n",
    "        exp_values= ablation_mapped_dict[a]\n",
    "        wave_id=row['wave_id'][0]\n",
    "        #print(a,wave_id,exp_values)\n",
    "        if (wave_id in exp_values) and (wave_id!='Llama2_base') and ('without_' not in wave_id) :\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_experiment_type(row):\n",
    "        a=row['exp']\n",
    "        if '1VAR' in a:\n",
    "            return 'one variable'\n",
    "        elif 'all' in a:\n",
    "            return 'all variables'\n",
    "        elif 'without' in a:\n",
    "            return 'all except one variable'\n",
    "        elif 'base' in a:\n",
    "            return 'no demographics'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_experiment_str(row):\n",
    "        a=row['exp']\n",
    "        if '1VAR' in a:\n",
    "            var= a.strip('1VAR_')\n",
    "            return f'{var} only'\n",
    "        elif 'all' in a:\n",
    "            return 'all variables'\n",
    "        elif 'without' in a:\n",
    "            var= a.strip('without_')\n",
    "            return  f'all except {var}'\n",
    "        elif 'base' in a:\n",
    "            return 'no demographics'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    population_JS= population_JS.T.reset_index().rename({'index':'exp',0:'js'},axis=1)\n",
    "    population_JS['exp_type']=population_JS.apply(get_experiment_type,axis=1)\n",
    "    population_JS['exp_str']=population_JS.apply(get_experiment_str,axis=1)\n",
    "    categories = ['no demographics','age only', 'berufabschluss only', 'eastwest only', 'gender only',\n",
    "           'party only', 'schulabschluss only', \n",
    "            'all except age', 'all except berufabschluss',\n",
    "           'all except eastwes', 'all except gender', 'all except party',\n",
    "           'all except schulabschluss','all variables']\n",
    "\n",
    "    population_JS['exp_str'] = pd.Categorical(population_JS['exp_str'], categories=categories, ordered=True)\n",
    "    population_JS=population_JS.sort_values(by='exp_str')\n",
    "    ordered_categories= ['no demographics','one variable', 'all variables', 'all except one variable'\n",
    "           ]\n",
    "    ablation_js_population=population_JS\n",
    "\n",
    "    fig = px.bar(\n",
    "        ablation_js_population,\n",
    "        x='exp',  \n",
    "        y='js',\n",
    "        color='exp_type',\n",
    "        category_orders={'exp_str': categories},\n",
    "    )\n",
    "\n",
    "\n",
    "    max_level = ablation_js_population['js'].max()\n",
    "    min_level = ablation_js_population['js'].min()\n",
    "    fig.add_shape(type=\"line\", x0=-0.5, y0=max_level, x1=len(ablation_js_population['exp'])-0.5, y1=max_level,\n",
    "                  line=dict(color=\"gray\", width=2, dash=\"dash\"))\n",
    "    fig.add_shape(type=\"line\", x0=-0.5, y0=min_level, x1=len(ablation_js_population['exp'])-0.5, y1=min_level,\n",
    "                  line=dict(color=\"gray\", width=2, dash=\"dash\"))\n",
    "\n",
    "    # Update x-axis tick labels\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Experiment Type',\n",
    "        yaxis_title='JS',\n",
    "        xaxis_tickvals=ablation_js_population['exp'],\n",
    "        xaxis_ticktext=ablation_js_population['exp']  # Display label for x-axis\n",
    "    )\n",
    "    fig.update_yaxes(dtick=0.05)\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Ablation Experiment : Experiment Types - JS Distance',\n",
    "        yaxis_title='JS'\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Experiment Type',\n",
    "        yaxis_title='JS',\n",
    "        xaxis_tickvals=list(range(len(ablation_js_population))),\n",
    "        xaxis_ticktext=ablation_js_population['exp_str']\n",
    "    )\n",
    "\n",
    "    #fig.write_html('abl_pop.html')\n",
    "    return fig\n",
    "f= get_ablation_JS_plot(population_JS)\n",
    "f.write_html('anan.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_dates={}\n",
    "for wave_id in range(12, 22):\n",
    "    wave_df, wave_open_ended_df, df_coding_840s = load_raw_survey_data(wave_id)\n",
    "    start_date = pd.to_datetime(wave_df.field_start.unique()[0]).strftime('%d-%m-%Y')\n",
    "    wave_dates[wave_id]=start_date\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_dates={12: '05-11-2019',\n",
    " 13: '21-04-2020',\n",
    " 14: '03-11-2020',\n",
    " 15: '25-02-2021',\n",
    " 16: '06-05-2021',\n",
    " 17: '07-07-2021',\n",
    " 18: '11-08-2021',\n",
    " 19: '15-09-2021',\n",
    " 20: '29-09-2021',\n",
    " 21: '09-12-2021'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w10=get_wave_demographics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bert.utils import get_experiment_df\n",
    "from src.paths import RESULTS_DIR\n",
    "from tqdm import tqdm\n",
    "\n",
    "classid2trainid = {int(classname):idx  for idx, classname in enumerate(sorted(pd.read_csv(os.path.join(CODING_DIR,'map.csv')).upperclass_id.unique())) }\n",
    "df_lookup= pd.read_csv(os.path.join(CODING_DIR,'map.csv'))\n",
    "label2str= dict(zip(df_lookup.upperclass_id,df_lookup.upperclass_name))\n",
    "label_names= [label2str[i] for i in range(0,len(label2str)) ]\n",
    "labels_16= [label_name for label_name in label_names if label_name!='LLM refusal']\n",
    "\n",
    "\n",
    "\n",
    "def get_wave_demographics(wave_number):\n",
    "    wave_df, wave_open_ended_df, df_coding_840s = load_raw_survey_data(wave_number)\n",
    "    wave_open_ended_df = process_open_ended_new(wave_open_ended_df, df_coding_840s, wave_number)\n",
    "    wave_df_processed = process_wave_data(wave_df, wave_open_ended_df, wave_number).filter(regex='lfdn$|gender|^age$|^age_groups$|clause|party|ostwest|user|labels|eastwest|text|output|highest_prob_label')\n",
    "    wave_df_processed['highest_prob_label']=wave_df_processed['highest_prob_label'].map(label2str)\n",
    "    age_bins = [17, 29, 44, 59, 74]\n",
    "    age_labels = [ '18-29 YEARS', '30-44 YEARS', '45-59 YEARS', '60 and more']\n",
    "    wave_df_processed.loc[:, 'age_groups'] = pd.cut(wave_df_processed['age'], bins=age_bins, labels=age_labels, right=True)\n",
    "    return wave_df_processed\n",
    "    \n",
    "def get_demographics_and_labels(wave_number,demographics):\n",
    "    \n",
    "    survey_labels_matrix=pd.DataFrame(demographics.labels.tolist(), columns=label_names  )\n",
    "    survey_labels_matrix.index=demographics.lfdn\n",
    "    survey_labels_matrix.drop(list(survey_labels_matrix.filter(regex='LLM refusal')), axis=1, inplace=True)\n",
    "    k=pd.merge(demographics, survey_labels_matrix, left_on='lfdn',right_on=survey_labels_matrix.index, how='inner')#.groupby('ostwest').apply(lambda x:)\n",
    "    return k\n",
    "\n",
    "def extract_model_predictions(row, model_name):\n",
    "    try:\n",
    "        for entry in row:\n",
    "            if entry['model_name'] == model_name:\n",
    "                pred = entry['result'].get('predictions', None)\n",
    "                return pred\n",
    "    except:\n",
    "        print(row, model_name)\n",
    "\n",
    "def get_highest_prob_label(row, model_name):\n",
    "    try:\n",
    "        for entry in row:\n",
    "            if entry['model_name'] == model_name:\n",
    "                classification_result = entry['result']\n",
    "                label_names = classification_result['pred_label_names']\n",
    "                label_probs = classification_result['pred_label_probs']\n",
    "\n",
    "                max_prob_index = label_probs.index(max(label_probs))\n",
    "\n",
    "                # Return the label name with the highest probability\n",
    "                return label_names[max_prob_index]\n",
    "        return None \n",
    "    except:\n",
    "        print(row, model_name)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "def get_demographics_and_llm_labels(wave_number,experiment_folder,wave_demographics):\n",
    "    df_exp= get_experiment_df(f'/dss/dsshome1/0F/ra46lup2/ma-yoztyurk/outputs/text_generations/{wave_number}/{experiment_folder}')\n",
    "    \n",
    "    df_exp= df_exp[df_exp.user_id.isin(wave_demographics['lfdn'].tolist() )]# to make sure numbers match,i.e  we dont put GLES-LLM inconclusive answers \n",
    "    \n",
    "    model_preds = df_exp['classification_coarse'].apply(lambda x: extract_model_predictions(x, model_name='bert_mixed_coarse_resample20240708_195103'))\n",
    "    llm_labels_matrix=  pd.DataFrame(model_preds.tolist(), columns=label_names )\n",
    "    llm_labels_matrix.index = df_exp.user_id\n",
    "    llm_labels_matrix.drop(list(llm_labels_matrix.filter(regex='LLM refusal')), axis=1, inplace=True)\n",
    "    \n",
    "    highest_prob_pred= df_exp['classification_coarse'].apply(lambda x: get_highest_prob_label(x, model_name='bert_mixed_coarse_resample20240708_195103'))\n",
    "    llm_labels_matrix['highest_prob_label_llm']=highest_prob_pred.values\n",
    "    k2=pd.merge(wave_demographics, llm_labels_matrix, left_on='lfdn',right_on=llm_labels_matrix.index, how='inner')#.groupby('ostwest').apply(lambda x:)\n",
    "    return k2\n",
    "\n",
    "def calculate_pmf_population(df):\n",
    "    filtered_columns = [col for col in df.columns if col in label_names]\n",
    "    column_sum= df[filtered_columns].sum(axis=0)\n",
    "    dist=column_sum/column_sum.sum().round(4)\n",
    "    return dist\n",
    "\n",
    "def calculate_pmf_by_groups(df):\n",
    "    filtered_columns = [col for col in df.columns if col in label_names]\n",
    "    dist_list=[]\n",
    "    for key in social_groups:\n",
    "        grouped = df.groupby(key)\n",
    "        sum_lbl = grouped[filtered_columns].sum()\n",
    "        dist = sum_lbl.div(sum_lbl.sum(axis=1),axis=0).round(4)\n",
    "        dist_list.append(dist)\n",
    "    \n",
    "    dist_df= pd.concat(dist_list)\n",
    "    return dist_df\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def get_js_dist_population(df1,df2):\n",
    "    js_dist_list=[]\n",
    "    js=distance.jensenshannon(df1,df2)\n",
    "    js_dist_list.append([js])\n",
    "    df=pd.DataFrame(js_dist_list,columns=['js'])\n",
    "    return  df\n",
    "\n",
    "def get_js_dist_by_groups(df1,df2):\n",
    "    assert df1.shape[0]==df2.shape[0]\n",
    "    js_dist_list=[]\n",
    "    for i in range(0,df1.shape[0]):\n",
    "            df1_group_i = df1.iloc[i]\n",
    "            df2_group_i = df2.iloc[i]\n",
    "            df1_groupname=df1_group_i.name\n",
    "            df2_groupname=df2_group_i.name\n",
    "            assert df1_groupname ==df2_groupname\n",
    "            js=distance.jensenshannon(df1_group_i,df2_group_i)\n",
    "            js_dist_list.append([df1_groupname,js])\n",
    "    df=pd.DataFrame(js_dist_list,columns=['social_group','js'])\n",
    "    df['social_group_category']=df['social_group'].map(social_group_to_category)\n",
    "    return  df\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "\n",
    "def concat_colnames_nonzero(row):\n",
    "    return '_'.join([col for col in row.index if row[col] != 0])\n",
    "\n",
    "            \n",
    "def get_MI_from_dataset( df ):\n",
    "    df.loc[:, 'labels_concatted'] = df[labels_16].apply(concat_colnames_nonzero,axis=1)\n",
    "    df= df.filter(regex='gender|^age_groups$|clause|party|ostwest|labels_concatted|eastwest')\n",
    "    df['combined_features'] = df.apply(lambda row: '_'.join(row.drop('labels_concatted').astype(str)), axis=1)\n",
    "    data = df\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_encoded = df.apply(label_encoder.fit_transform)\n",
    "\n",
    "    X = df_encoded#.drop('first_label_categorical', axis=1)\n",
    "    y = df_encoded['labels_concatted']\n",
    "\n",
    "    mi_scores = mutual_info_classif(X, y,discrete_features=True)\n",
    "\n",
    "    mi_results = pd.DataFrame({'Feature': X.columns, 'MI Score': mi_scores/mi_scores.max()})\n",
    "    return mi_results\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_experiments= ['1VAR_age',\n",
    " '1VAR_berufabschluss',\n",
    " '1VAR_eastwest',\n",
    " '1VAR_gender',\n",
    " '1VAR_party',\n",
    " '1VAR_schulabschluss',\n",
    " 'Llama2_all',\n",
    " 'Llama2_base',\n",
    " 'without_age',\n",
    " 'without_berufabschluss',\n",
    " 'without_eastwest',\n",
    " 'without_gender',\n",
    " 'without_party',\n",
    " 'without_schulabschluss']\n",
    "\n",
    "\n",
    "model_comparison_experiments= [\n",
    " 'google-gemma-7b-it_12_1712704376_modified',\n",
    " 'Llama2_all',\n",
    " 'mistralai-Mixtral-8x7B-Instruct-v0.1_12_1712772173'\n",
    "]\n",
    "\n",
    "wave_experiments= ['12/Llama2_all',\n",
    " '13/Llama2_all',\n",
    " '14/Llama2_all',\n",
    " '15/Llama2_all',\n",
    " '16/Llama2_all',\n",
    " '17/Llama2_all',\n",
    " '18/Llama2_all',\n",
    " '19/Llama2_all',\n",
    " '20/Llama2_all',\n",
    " '21/Llama2_all']\n",
    "\n",
    "social_groups=[ 'ostwest','berufabschluss_clause', 'leaning_party', 'gender','schulabschluss_clause', 'age_groups']\n",
    "\n",
    "social_category_to_group={'ostwest': ['Westdeutschland', 'Ostdeutschland'],\n",
    " 'berufabschluss_clause': ['hat einen Berufsfachschulabschluss.',\n",
    "  'hat einen Fachhochschulabschluss.',\n",
    "  'hat einen Universitätsabschluss.',\n",
    "  'hat eine kaufmännische Lehre abgeschlossen.',\n",
    "  'hat einen Meisterabschluss oder Technikerabschluss.',\n",
    "  'hat eine Lehre abgeschlossen.',\n",
    "  'hat keine berufliche Ausbildung abgeschlossen.',\n",
    "  'hat einen Fachschulabschluss.',\n",
    "  'befindet sich noch in beruflicher Ausbildung.',\n",
    "  'hat ein Berufliches Praktikum oder Volontariat abgeschlossen.',\n",
    "  'hat eine gewerbliche oder landwirtschaftliche Lehre abgeschlossen.'],\n",
    " 'leaning_party': ['die Grünen',\n",
    "  'Die Linke',\n",
    "  'die CDU/CSU',\n",
    "  'die FDP',\n",
    "  'die SPD',\n",
    "  'die AfD',\n",
    "  'keine Partei',\n",
    "  'eine Kleinpartei'],\n",
    " 'gender': ['weiblich', 'männlich'],\n",
    " 'schulabschluss_clause': ['hat einen Fachhochschulreife',\n",
    "  'hat das Abitur',\n",
    "  'hat einen Realschulabschluss',\n",
    "  'hat einen Hauptschulabschluss',\n",
    "  'hat keinen Schulabschluss',\n",
    "  'ist noch Schüler/in'],\n",
    " 'age_groups': ['45-59 YEARS', '60 and more', '30-44 YEARS', '18-29 YEARS']}\n",
    "social_group_to_category = {v: k for k, vals in social_category_to_group.items() for v in vals}\n",
    "\n",
    "def save_waveExperiment_pmf(survey_population_df,llm_population_df,survey_group_pmf,llm_group_pmf): \n",
    "    survey_population_df.to_csv(os.path.join(RESULTS_DIR,'waveExperiment','survey_population_level_pmf_wave12_to_wave21.csv'),index=True)\n",
    "    llm_population_df.to_csv(os.path.join(RESULTS_DIR,'waveExperiment','llm_population_level_pmf_wave12_to_wave21.csv'),index=True)\n",
    "\n",
    "    \n",
    "    for key, df in survey_group_pmf.items(): #saving social group level pmf of each wave seperately.\n",
    "        fname=f'survey_group_pmf_wave{key}.csv'\n",
    "        df.to_csv(os.path.join(RESULTS_DIR,'waveExperiment',fname), index=True)\n",
    "        \n",
    "    for key, df in llm_group_pmf.items():\n",
    "        fname=f'llm_group_pmf_wave{key}.csv'\n",
    "        df.to_csv(os.path.join(RESULTS_DIR,'waveExperiment',fname), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmf_waveExperiment(save=True,until=22):\n",
    "    survey_labels_dict={}\n",
    "    llm_labels_dict={}\n",
    "\n",
    "    survey_population_pmf={}\n",
    "    llm_population_pmf={}\n",
    "\n",
    "    survey_group_pmf={}\n",
    "    llm_group_pmf={}\n",
    "\n",
    "    for wave_number in range(12,until):\n",
    "        demographics= get_wave_demographics(wave_number)\n",
    "\n",
    "        survey_labels = get_demographics_and_labels(wave_number,demographics)\n",
    "        llm_labels = get_demographics_and_llm_labels(wave_number,'Llama2_all',demographics)\n",
    "\n",
    "        survey_labels_dict[wave_number]=survey_labels\n",
    "        llm_labels_dict[wave_number]=llm_labels\n",
    "        \n",
    "        survey_population_pmf[wave_number] =  calculate_pmf_population(survey_labels)\n",
    "        llm_population_pmf[wave_number] =     calculate_pmf_population(llm_labels)\n",
    "        \n",
    "\n",
    "        survey_group_pmf[wave_number] =  calculate_pmf_by_groups(survey_labels)\n",
    "        llm_group_pmf[wave_number] =     calculate_pmf_by_groups(llm_labels)\n",
    "        \n",
    "    survey_population_df=pd.DataFrame(survey_population_pmf)\n",
    "    llm_population_df=pd.DataFrame(llm_population_pmf)\n",
    "        \n",
    "    if save==True:\n",
    "        save_waveExperiment_pmf(survey_population_df,llm_population_df,survey_group_pmf,llm_group_pmf)\n",
    "    \n",
    "    return  survey_labels_dict,llm_labels_dict,survey_population_df,llm_population_df,survey_group_pmf,llm_group_pmf\n",
    "#survey_labels_dict,llm_labels_dict = get_pmf_waveExperiment()\n",
    "#save_waveExperiment_pmf(survey_population_df,llm_population_df,survey_group_pmf,llm_group_pmf)\n",
    "\n",
    "def read_waveExperiment_pmf(): \n",
    "    try:\n",
    "        survey_population_df=pd.read_csv(os.path.join(RESULTS_DIR,'waveExperiment','survey_population_level_pmf_wave12_to_wave21.csv'),index_col=0)\n",
    "        llm_population_df=pd.read_csv(os.path.join(RESULTS_DIR,'waveExperiment','llm_population_level_pmf_wave12_to_wave21.csv'),index_col=0)\n",
    "    except:\n",
    "            print('Couldnt find survey_population_df/llm_population_df')\n",
    "    survey_group_pmf={}\n",
    "    llm_group_pmf={}\n",
    "    \n",
    "    for key in list(range(12,22)):\n",
    "        try:\n",
    "            fname=f'survey_group_pmf_wave{key}.csv'\n",
    "            survey_group_pmf[key] = pd.read_csv(os.path.join(RESULTS_DIR,'waveExperiment',fname),index_col=0)\n",
    "\n",
    "        except:\n",
    "            print(f'Couldnt find file {fname}')\n",
    "        try:\n",
    "            fname=f'llm_group_pmf_wave{key}.csv'\n",
    "            llm_group_pmf[key] = pd.read_csv(os.path.join(RESULTS_DIR,'waveExperiment',fname),index_col=0)\n",
    "        except:\n",
    "            print(f'Couldnt find file {fname}')\n",
    "    return survey_population_df,llm_population_df,survey_group_pmf,llm_group_pmf\n",
    "\n",
    "#survey_population_df,llm_population_df,survey_group_pmf,llm_group_pmf=read_waveExperiment_pmf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time \n",
    "begin=time.time()\n",
    "survey_labels_dict,llm_labels_dict,survey_population_df,llm_population_df,survey_group_pmf,llm_group_pmf = get_pmf_waveExperiment(until=22)\n",
    "end=time.time()\n",
    "print(end-begin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "df=survey_labels_dict[12].filter(regex=f'gender|^age_groups$|clause|party|ostwest|highest_prob_label|eastwest')\n",
    "X = df_encoded.drop(columns=['highest_prob_label'])\n",
    "Y = df['highest_prob_label']\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "encoder = OneHotEncoder(drop='first')\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "#print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = encoder.get_feature_names_out()\n",
    "feature_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "#print(\"Feature Importances:\\n\", feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "df=survey_labels_dict[12].filter(regex=f'gender|^age_groups$|clause|party|ostwest|highest_prob_label|eastwest')\n",
    "X = df_encoded.drop(columns=['highest_prob_label'])\n",
    "Y = df['highest_prob_label']\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "encoder = OneHotEncoder(drop='first')\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "#print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = encoder.get_feature_names_out()\n",
    "feature_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "#print(\"Feature Importances:\\n\", feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_train, rf_model.predict(X_train)))\n",
    "print(\"Classification Report:\\n\", classification_report(y_train, rf_model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dist= pd.Series(y_pred).value_counts(1)\n",
    "pred_dist= pred_dist.reindex(label_names, fill_value=0) \n",
    "\n",
    "test_dist=pd.Series(y_test).value_counts(1)\n",
    "test_dist= test_dist.reindex(label_names, fill_value=0) \n",
    "\n",
    "distance.jensenshannon( pred_dist, test_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.formula.api import mnlogit\n",
    "from patsy import dmatrices\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df=survey_labels_dict[12].filter(regex=f'gender|^age_groups$|clause|leaning_party|ostwest|highest_prob_label|ostwest')\n",
    "# # One-hot encode the categorical variables\n",
    "# df_encoded = pd.get_dummies(df, columns=['ostwest','age_groups', 'gender', 'berufabschluss_clause','schulabschluss_clause', 'leaning_party'], drop_first=True)\n",
    "\n",
    "# # Define the features and the target\n",
    "# X = df_encoded.drop(columns=['highest_prob_label'])\n",
    "# y = pd.get_dummies(df['highest_prob_label'], drop_first=True).astype(int) #df['highest_prob_label']\n",
    "\n",
    "\n",
    "df= df.head(1000)\n",
    "\n",
    "# Encode the dependent variable\n",
    "le = LabelEncoder()\n",
    "df['Y_encoded'] = le.fit_transform(df['highest_prob_label'])\n",
    "\n",
    "# Create the formula for the model\n",
    "formula = 'Y_encoded ~  C(age_groups)'\n",
    "\n",
    "# Fit the model\n",
    "model = mnlogit(formula, df)\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary of the results\n",
    "print(results.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df=survey_labels_dict[12].filter(regex=f'gender|^age_groups$|clause|party|ostwest|highest_prob_label|eastwest')\n",
    "# One-hot encode the categorical variables\n",
    "df_encoded = pd.get_dummies(df, columns=['ostwest','age_groups', 'gender', 'berufabschluss_clause','schulabschluss_clause', 'leaning_party'], drop_first=True)\n",
    "\n",
    "# Define the features and the target\n",
    "X = df_encoded.drop(columns=['highest_prob_label'])\n",
    "y = pd.get_dummies(df['highest_prob_label'], drop_first=True).astype(int) #df['highest_prob_label']\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = sm.Logit(y, X.astype(int)).fit() \n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_group_pmf[12]\n",
    "for social_group in survey_group_pmf[12].index.values: \n",
    "    for wave_id in range(12,22):#survey_group_pmf.keys():\n",
    "        js=distance.jensenshannon(survey_group_pmf[wave_id].loc[social_group],survey_group_pmf[20].loc[social_group])\n",
    "        js2=distance.jensenshannon(survey_group_pmf[20].loc[social_group],llm_group_pmf[20].loc[social_group])\n",
    "        print(wave_id,social_group,js,js2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in survey_population_df:\n",
    "    js=distance.jensenshannon(survey_population_df[col],survey_population_df[21])\n",
    "    print(col,js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in survey_population_df:\n",
    "    js=distance.jensenshannon(survey_population_df[col],survey_population_df[20])\n",
    "    \n",
    "    print(col,js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in survey_population_df:\n",
    "    js=distance.jensenshannon(llm_population_df[col],survey_population_df[col])\n",
    "    print(col,js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score,accuracy_score\n",
    "def get_population_acc_ck(llm_labels_dict):\n",
    "    for i in llm_labels_dict.keys():\n",
    "        acc=accuracy_score(llm_labels_dict[i]['highest_prob_label'], llm_labels_dict[i]['highest_prob_label_llm'])\n",
    "        ck=cohen_kappa_score(llm_labels_dict[i]['highest_prob_label'], llm_labels_dict[i]['highest_prob_label_llm'])\n",
    "        print(acc,ck)\n",
    "        \n",
    "get_population_acc_ck(llm_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_cc_js for wave_groups\n",
    "# from sklearn.metrics import cohen_kappa_score,accuracy_score\n",
    "# d_list=[]\n",
    "# for label in labels_16:\n",
    "#     for i in range (12,22):\n",
    "#         acc=accuracy_score(survey_labels_dict[i][label], llm_labels_dict[i][label])\n",
    "#         cc=cohen_kappa_score(survey_labels_dict[i][label], llm_labels_dict[i][label])\n",
    "#         js=distance.jensenshannon(survey_labels_dict[i][label].value_counts(1),llm_labels_dict[i][label].value_counts(1))\n",
    "#         d={'acc':acc,'cc':cc,'js':js,'wave':i,'label':label}\n",
    "#         d_list.append(d)\n",
    "# pd.DataFrame(d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_JS_as_multiclass(df):\n",
    "    a=df['highest_prob_label_llm'].value_counts(1)\n",
    "    b=df['highest_prob_label'].value_counts(1)\n",
    "    c=pd.concat([a,b],axis=1).fillna(0)\n",
    "    return distance.jensenshannon(c.iloc[:,0],c.iloc[:,1])\n",
    "def calculate_JS_as_multiclass_waveExperiment(llm_labels_dict):\n",
    "    js_multiclass={}\n",
    "    for wave_id in llm_labels_dict.keys():\n",
    "        js_multiclass[wave_id]=calculate_JS_as_multiclass()\n",
    "    return pd.DataFrame([js_multiclass])\n",
    "calculate_JS_as_multiclass_waveExperiment(llm_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_label_from_strata(df):\n",
    "    df= survey_labels_dict[13][['ostwest',\n",
    "     'berufabschluss_clause',\n",
    "     'leaning_party',\n",
    "     'gender',\n",
    "     'schulabschluss_clause',\n",
    "     'age_groups',\n",
    "     'highest_prob_label']]\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Assuming your dataframe is named 'df'\n",
    "\n",
    "    # Step 1: Group by all columns except highest_prob_label and highest_prob_label_sampled\n",
    "    grouping_columns = [col for col in df.columns if col not in ['highest_prob_label', 'highest_prob_label_sampled']]\n",
    "\n",
    "    # Step 2 & 3: Get unique values and their frequencies, create probability distribution\n",
    "    def sample_label(group):\n",
    "        value_counts = group['highest_prob_label'].value_counts(normalize=True)\n",
    "        return pd.Series(np.random.choice(value_counts.index, p=value_counts.values, size=len(group)), index=group.index)\n",
    "\n",
    "    # Step 4: Sample a new value for each group\n",
    "    df['new_sampled_label'] = df.groupby(grouping_columns, group_keys=False).apply(sample_label)\n",
    "    return df \n",
    "# Display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_acc_group(df):\n",
    "#     for feature, val_list in social_category_to_group.items():\n",
    "#         for val in val_list:\n",
    "#             a=df[df[feature]==val]\n",
    "#             acc= accuracy_score(a['highest_prob_label'], a['highest_prob_label_llm'])\n",
    "#             cc=cohen_kappa_score(a['highest_prob_label'], a['highest_prob_label_llm'])\n",
    "#             print(val,acc)\n",
    "# calc_acc_group(llm_labels_dict[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveExperiment_Entropy(survey_population_df,llm_population_df,survey_group_pmf,llm_group_pmf):\n",
    "    \n",
    "    def calculate_population_entropy(df):\n",
    "        entropy_dict={}\n",
    "        for col in df.columns:\n",
    "            H = entropy(df[col], base = 2)\n",
    "            entropy_dict[col]=H\n",
    "        return entropy_dict\n",
    "    def calculate_group_entropy(df):\n",
    "        return dict(zip(df.index,entropy(df,base=2,axis=1)))\n",
    "\n",
    "    def combine_results(survey_dict,llm_dict):\n",
    "        df1 = pd.DataFrame([survey_dict]).T\n",
    "        df1['study']=df1.index\n",
    "        df1['source'] = 'survey'\n",
    "\n",
    "        df2 = pd.DataFrame([llm_dict]).T\n",
    "        df2['study']=df2.index\n",
    "        df2['source'] = 'llm'\n",
    "        # Concatenate the DataFrames\n",
    "        combined_results = pd.concat([df1, df2], ignore_index=True)\n",
    "        combined_results= combined_results.rename({0:'shannon_entropy'},axis=1)\n",
    "        return combined_results\n",
    "    \n",
    "    def get_dict_to_df(json_data,source):\n",
    "        data_list = []\n",
    "        for wave_id, social_groups in json_data.items():\n",
    "            for social_group, entropy in social_groups.items():\n",
    "                data_list.append([wave_id, social_group, entropy])\n",
    "        df = pd.DataFrame(data_list, columns=['wave_id', 'social_group', 'entropy'])\n",
    "        df['source']=source\n",
    "        return df\n",
    "    ###population-level results\n",
    "    survey_population_entropy=calculate_population_entropy(survey_population_df)\n",
    "    llm_population_entropy=calculate_population_entropy(llm_population_df)\n",
    "    population_level_entropy_results= combine_results(survey_population_entropy,llm_population_entropy)\n",
    "    ###population-level results\n",
    "\n",
    "    \n",
    "    ###group-level results\n",
    "    survey_group_entropy_dict={}\n",
    "    llm_group_entropy_dict={}\n",
    "    assert survey_group_pmf.keys()==llm_group_pmf.keys()\n",
    "    for i in survey_group_pmf.keys():\n",
    "        survey_group_entropy_dict[i]=calculate_group_entropy(survey_group_pmf[i])\n",
    "        llm_group_entropy_dict[i]=calculate_group_entropy(llm_group_pmf[i])\n",
    "    group_level_entropy_results = pd.concat([get_dict_to_df(survey_group_entropy_dict,source='survey'),get_dict_to_df(llm_group_entropy_dict,source='llm')], ignore_index=True)\n",
    "    ###group-level results\n",
    "\n",
    "    return population_level_entropy_results,group_level_entropy_results\n",
    "\n",
    "def get_JS_waveExperiment(survey_population_df,llm_population_df,survey_group_pmf,llm_group_pmf):\n",
    "    \n",
    "    def get_groupedJS_waveExperiment(survey_group_pmf,llm_group_pmf):\n",
    "        assert survey_group_pmf.keys()==llm_group_pmf.keys()\n",
    "        js_dfs=[]\n",
    "        for i in survey_group_pmf.keys():\n",
    "            df= get_js_dist_by_groups(survey_group_pmf[i],llm_group_pmf[i])\n",
    "            df.loc[:,'wave']=i\n",
    "            js_dfs.append(df)\n",
    "\n",
    "        return  pd.concat(js_dfs)\n",
    "\n",
    "\n",
    "    group_JS= get_groupedJS_waveExperiment(survey_group_pmf,llm_group_pmf)\n",
    "    population_JS=get_js_dist_population(survey_population_df,llm_population_df)\n",
    "    return population_JS,group_JS\n",
    "\n",
    "def get_MI_experiment(survey_labels_dict,llm_labels_dict):\n",
    "    wave_ids= survey_labels_dict.keys()\n",
    "    dfs=[]\n",
    "    for wave_id in wave_ids:\n",
    "        mi_survey=get_MI_from_dataset(survey_labels_dict[wave_id])\n",
    "        mi_survey['wave']=wave_id\n",
    "        mi_survey['source']='survey'\n",
    "        mi_llm= get_MI_from_dataset(llm_labels_dict[wave_id])\n",
    "        mi_llm['wave']=wave_id\n",
    "        mi_llm['source']='llm'\n",
    "        dfs.append(mi_survey)\n",
    "        dfs.append(mi_llm)\n",
    "        \n",
    "    return pd.concat(dfs)\n",
    "\n",
    "from dython.nominal import associations\n",
    "\n",
    "def get_cramerV(df):\n",
    "    df.loc[:, 'labels_concatted'] = df[labels_16].apply(concat_colnames_nonzero,axis=1)\n",
    "    df= df.filter(regex='gender|^age_groups$|clause|party|ostwest|labels_concatted|eastwest')\n",
    "    #df['combined_features'] = df.apply(lambda row: '_'.join(row.drop('labels_concatted').astype(str)), axis=1)\n",
    "    return associations(df,compute_only =True)['corr']['labels_concatted']\n",
    "def get_cramerV2(df,y_col):\n",
    "    df= df.filter(regex=f'gender|^age_groups$|clause|party|ostwest|{y_col}|eastwest')\n",
    "    return associations(df,compute_only =True)['corr'][f'{y_col}']\n",
    "\n",
    "\n",
    "def get_cramerV_waveExperiment2(survey_labels_dict,llm_labels_dict):\n",
    "    results=[]\n",
    "    for wave_id in survey_labels_dict.keys():\n",
    "        s_cramer= pd.DataFrame(get_cramerV2(survey_labels_dict[wave_id],y_col='highest_prob_label')).rename({'highest_prob_label':'Cramers\\' V'},axis=1)\n",
    "        s_cramer['source']='survey'\n",
    "        s_cramer['wave_id']=wave_id\n",
    "        \n",
    "        l_cramer= pd.DataFrame(get_cramerV2(llm_labels_dict[wave_id],y_col='highest_prob_label_llm')).rename({'highest_prob_label_llm':'Cramers\\' V'},axis=1)\n",
    "        l_cramer['source']='llm'\n",
    "        l_cramer['wave_id']=wave_id\n",
    "        \n",
    "        results.append(s_cramer)\n",
    "        results.append(l_cramer)\n",
    "    \n",
    "    results_df=pd.concat(results)\n",
    "    results_df=results_df.reset_index()\n",
    "    results_df=results_df.query(\"index!='highest_prob_label_llm'\").query(\"index!='highest_prob_label'\")\n",
    "    return results_df\n",
    "\n",
    "def get_cramerV_waveExperiment(survey_labels_dict,llm_labels_dict):\n",
    "    results=[]\n",
    "    for wave_id in survey_labels_dict.keys():\n",
    "        s_cramer= pd.DataFrame(get_cramerV(survey_labels_dict[wave_id])).rename({'labels_concatted':'Cramers\\' V'},axis=1)\n",
    "        s_cramer['source']='survey'\n",
    "        s_cramer['wave_id']=wave_id\n",
    "        \n",
    "        l_cramer= pd.DataFrame(get_cramerV(llm_labels_dict[wave_id])).rename({'labels_concatted':'Cramers\\' V'},axis=1)\n",
    "        l_cramer['source']='llm'\n",
    "        l_cramer['wave_id']=wave_id\n",
    "        \n",
    "        results.append(s_cramer)\n",
    "        results.append(l_cramer)\n",
    "    \n",
    "    results_df=pd.concat(results)\n",
    "    results_df=results_df.reset_index()\n",
    "    results_df=results_df.query(\"index!='labels_concatted'\")\n",
    "    return results_df\n",
    "\n",
    "\n",
    "\n",
    "MI_results_waveExperiment= get_MI_experiment(survey_labels_dict,llm_labels_dict)\n",
    "population_JS,group_JS= get_JS_waveExperiment(survey_population_df,llm_population_df,survey_group_pmf,llm_group_pmf)\n",
    "population_level_entropy_results,group_level_entropy_results = get_waveExperiment_Entropy(survey_population_df,llm_population_df,survey_group_pmf,llm_group_pmf)\n",
    "cramer_results=get_cramerV_waveExperiment(survey_labels_dict,llm_labels_dict)\n",
    "cramer_results2=get_cramerV_waveExperiment2(survey_labels_dict,llm_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy_JS_corr_data(group_JS,group_level_entropy_results)\n",
    "    shortened_dict = {#for plotting\n",
    "        '18-29 YEARS': '18-29',\n",
    "        '30-44 YEARS': '30-44',\n",
    "        '45-59 YEARS': '45-59',\n",
    "        '60 and more': '60+',\n",
    "        'Die Linke': 'Linke',\n",
    "        'Ostdeutschland': 'Ostdeutschland',\n",
    "        'Westdeutschland': 'Westdeutschland',\n",
    "        'befindet sich noch in beruflicher Ausbildung.': 'beruflicher Ausbildung',\n",
    "        'die AfD': 'AfD',\n",
    "        'die CDU/CSU': 'CDU/CSU',\n",
    "        'die FDP': 'FDP',\n",
    "        'die Grünen': 'Grünen',\n",
    "        'die SPD': 'SPD',\n",
    "        'eine Kleinpartei': 'Kleinpartei',\n",
    "        'hat das Abitur': 'Abitur',\n",
    "        'hat ein Berufliches Praktikum oder Volontariat abgeschlossen.': 'Berufliches Praktikum/Volontariat',\n",
    "        'hat eine Lehre abgeschlossen.': 'Lehre abgeschlossen',\n",
    "        'hat eine gewerbliche oder landwirtschaftliche Lehre abgeschlossen.': 'gewerbliche/landwirtschaftliche Lehre',\n",
    "        'hat eine kaufmännische Lehre abgeschlossen.': 'kaufmännische Lehre',\n",
    "        'hat einen Berufsfachschulabschluss.': 'Berufsfachschulabschluss',\n",
    "        'hat einen Fachhochschulabschluss.': 'Fachhochschulabschluss',\n",
    "        'hat einen Fachhochschulreife': 'Fachhochschulreife',\n",
    "        'hat einen Fachschulabschluss.': 'Fachschulabschluss',\n",
    "        'hat einen Hauptschulabschluss': 'Hauptschulabschluss',\n",
    "        'hat einen Meisterabschluss oder Technikerabschluss.': 'Meister-/Technikerabschluss',\n",
    "        'hat einen Realschulabschluss': 'Realschulabschluss',\n",
    "        'hat einen Universitätsabschluss.': 'Universitätsabschluss',\n",
    "        'hat keine berufliche Ausbildung abgeschlossen.': 'keine berufliche Ausbildung',\n",
    "        'hat keinen Schulabschluss': 'kein Schulabschluss',\n",
    "        'keine Partei': 'keine Partei',\n",
    "        'männlich': 'männlich',\n",
    "        'weiblich': 'weiblich'\n",
    "    }\n",
    "    k=pd.merge(group_JS,group_level_entropy_results.query(\"source=='survey'\"),left_on=['wave','social_group'],right_on=['wave_id','social_group'])\n",
    "    k=k.groupby(['social_group','social_group_category'])[['js','entropy']].mean().reset_index()\n",
    "    k=k.query(\"social_group!='ist noch Schüler/in' \")\n",
    "    k.loc[:,'text']=k['social_group'].map(shortened_dict)\n",
    "    k=k.sort_values(by='entropy')\n",
    "\n",
    "    \n",
    "    k['social_group'].map(shortened_dict)\n",
    "    return k\n",
    "k=get_entropy_JS_corr_data(group_JS,group_level_entropy_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import linregress\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_entropy_JSDist_corr(k):\n",
    "    # Define marker symbols\n",
    "    symbols = ['circle', 'square', 'diamond', 'cross', 'x', 'star', 'circle-cross', 'cross-thin', 'diamond-tall', 'square-cross', 'hexagon', 'asterisk']\n",
    "    colors = px.colors.qualitative.Set2  # Fixed colors for each subplot\n",
    "\n",
    "    # Assign unique symbols within each social_group_category\n",
    "    symbol_map = {}\n",
    "    for category in k['social_group_category'].unique():\n",
    "        symbol_map[category] = symbols[:k[k['social_group_category'] == category].shape[0]]\n",
    "\n",
    "    # Create subplots\n",
    "    categories = k['social_group_category'].unique()\n",
    "    num_categories = len(categories)\n",
    "    rows = (num_categories + 2) // 3  # 2 columns layout\n",
    "\n",
    "    fig = make_subplots(rows=rows, cols=3, subplot_titles=[cat for cat in categories], vertical_spacing=0.1, horizontal_spacing=0.05)\n",
    "\n",
    "    # Add scatter plots for each social_group_category\n",
    "    for i, category in enumerate(categories):\n",
    "        row = i // 3 + 1\n",
    "        col = i % 3 + 1\n",
    "        category_data = k[k['social_group_category'] == category]\n",
    "\n",
    "        # Calculate linear regression for trendline\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(category_data['entropy'], category_data['js'])\n",
    "        r_squared = r_value ** 2  # Calculate R²\n",
    "\n",
    "        # Add scatter trace\n",
    "        for j in range(len(category_data)):\n",
    "            position = 'middle right'  # Default text position\n",
    "            font_size = 12  # Default font size\n",
    "            if category == 'berufabschluss_clause':\n",
    "                font_size = 10  # Reduced font size for specific category\n",
    "            if j == len(category_data) - 1:  # Last dot\n",
    "                position = 'middle left'  # Adjust text position for last point\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[category_data['entropy'].iloc[j]],\n",
    "                    y=[category_data['js'].iloc[j]],\n",
    "                    mode='markers+text',\n",
    "                    text=[category_data['text'].iloc[j]],\n",
    "                    marker=dict(size=18, symbol=symbol_map[category][0], color=colors[i % len(colors)]),  # Fixed color for each category\n",
    "                    textposition=position,\n",
    "                    textfont=dict(size=font_size)\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col\n",
    "            )\n",
    "\n",
    "        # Add trendline\n",
    "        x_range = [category_data['entropy'].min(), category_data['entropy'].max()]\n",
    "        y_range = [intercept + slope * x for x in x_range]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_range,\n",
    "                y=y_range,\n",
    "                mode='lines',\n",
    "                name='Trendline',\n",
    "                line=dict(color='gray', dash='dash')  # Dashed line\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col\n",
    "        )\n",
    "\n",
    "\n",
    "        # Update x and y axis labels\n",
    "        fig.update_xaxes(title_text='Subgroup\\'s Survey Entropy', row=row, col=col)\n",
    "        fig.update_yaxes(title_text='JS Distance', row=row, col=col)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(title_text='Correlation Scatter Plots by Social Group Category', showlegend=False)\n",
    "\n",
    "    # Show plot\n",
    "    fig.write_html(\"subplots_corr_coef3.html\")\n",
    "plot_entropy_JSDist_corr(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_labels_dict2={}\n",
    "survey_population_pmf={}\n",
    "\n",
    "for wave_number in range(10,22):\n",
    "        demographics= get_wave_demographics(wave_number)\n",
    "        survey_labels = get_demographics_and_labels(wave_number,demographics)\n",
    "        survey_labels_dict2[wave_number]=survey_labels\n",
    "        survey_population_pmf[wave_number] =  calculate_pmf_population(survey_labels)\n",
    "        survey_population_df=pd.DataFrame(survey_population_pmf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "base = 2  # work in units of bits\n",
    "pk = np.array([1/2, 1/2])  # fair coin\n",
    "H = entropy(pk, base=base)\n",
    "a=entropy(pk=llm_population_df[~llm_population_df.index.str.contains('keine Angabe|weiß nich')], qk=survey_population_df[~survey_population_df.index.str.contains('keine Angabe|weiß nich')],  base=2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js_waves_between(df):\n",
    "    for i in range(len(df.columns) - 1):\n",
    "        p = df.iloc[:, i].values\n",
    "        q = df.iloc[:, i + 1].values\n",
    "        print(i,distance.jensenshannon(p,q))\n",
    "\n",
    "js_waves_between(llm_population_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js_waves_between(df):\n",
    "    for i in range(len(df.columns) - 1):\n",
    "        p = df.iloc[:, i].values\n",
    "        q = df.iloc[:, i + 1].values\n",
    "        print(i,distance.jensenshannon(p,q))\n",
    "\n",
    "js_waves_between(survey_population_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def visualize_cramer_waveExperiment_1x6(df,fname='cramer_waveExperiment_1x6.html'):\n",
    "    '''\n",
    "    visualize in 1x6 , for the digital use \n",
    "    '''\n",
    "    df['index'] = df['index'].astype('category')\n",
    "    df['x'] = df['index'].cat.codes * 0.3 + df['wave_id'] * 0.025\n",
    "\n",
    "    xtick_position = [df[df['index'] == x_val].x.median() for x_val in df['index'].unique()]\n",
    "    x_mapping = {code: label for code, label in zip(xtick_position, df['index'].unique())}\n",
    "    color_map = {\n",
    "        'survey': 'blue',  # Change 'source_1' to your actual source names\n",
    "        'llm': 'orange'  # Change 'source_2' to your actual source names\n",
    "    }\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for source in df['source'].unique():\n",
    "        filtered_df = df[df['source'] == source]\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=filtered_df['x'],\n",
    "            y=filtered_df['Cramers\\' V'],\n",
    "            mode='markers+text',\n",
    "            text=filtered_df['wave_id'],\n",
    "            textposition='top center',\n",
    "            name=source,\n",
    "            marker=dict(size=10, color=color_map.get(source, 'gray')),\n",
    "            hoverinfo='text'\n",
    "        ))\n",
    "\n",
    "        for index_val in filtered_df['index'].unique():\n",
    "            index_points = filtered_df[filtered_df['index'] == index_val]\n",
    "            if len(index_points) > 1:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=index_points['x'],\n",
    "                    y=index_points['Cramers\\' V'],\n",
    "                    mode='lines',\n",
    "                    line=dict(width=2, color=color_map.get(source, 'gray')),\n",
    "                    name=f'{source} - {index_val} Connection',\n",
    "                    showlegend=False\n",
    "                ))\n",
    "\n",
    "    # Customizing the layout\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Feature',\n",
    "        yaxis_title='Cramer\\'s V',\n",
    "        title='Cramers V for Prompt Features',\n",
    "        xaxis=dict(\n",
    "            tickmode='array',\n",
    "            tickvals=list(x_mapping.keys()),\n",
    "            ticktext=list(x_mapping.values())\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            range=[0, df['Cramers\\' V'].max() + 0.01],\n",
    "            tick0=0,\n",
    "            dtick=0.01\n",
    "        ),\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        legend=dict(\n",
    "            title='Source',\n",
    "            font=dict(size=14),\n",
    "            x=0.01,\n",
    "            y=0.99,\n",
    "            xanchor='left',\n",
    "            yanchor='top',\n",
    "            bgcolor='rgba(255, 255, 255, 0)',\n",
    "            bordercolor='black',\n",
    "            borderwidth=1\n",
    "        )\n",
    "    )\n",
    "    fig.update_yaxes(gridcolor='lightgray', gridwidth=0.5, griddash='dash')\n",
    "    fig.write_html(fname)\n",
    "    return fig\n",
    "\n",
    "fig= visualize_cramer_waveExperiment_1x6(cramer_results,'cramer_results.html')    \n",
    "fig= visualize_cramer_waveExperiment_1x6(cramer_results2,'cramer_results2.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def visualize_cramer_waveExperiment_2x3(df):\n",
    "    '''\n",
    "    visualize in 2x3 , for the printing\n",
    "    '''\n",
    "    df['index'] = df['index'].astype('category')\n",
    "    df['x'] = df['index'].cat.codes * 0.3 + df['wave_id'] * 0.025\n",
    "\n",
    "    unique_indices = df['index'].unique()\n",
    "    num_cols = 3\n",
    "    num_rows = (len(unique_indices) + num_cols - 1) // num_cols  # Calculate rows needed\n",
    "\n",
    "    # Create subplots\n",
    "    fig = make_subplots(rows=num_rows, cols=num_cols, shared_yaxes=True)\n",
    "\n",
    "    color_map = {\n",
    "        'survey': 'blue',\n",
    "        'llm': 'orange'\n",
    "    }\n",
    "\n",
    "    # Add traces for each index\n",
    "    for i, index_val in enumerate(unique_indices):\n",
    "        row = i // num_cols + 1\n",
    "        col = i % num_cols + 1\n",
    "        for source in df['source'].unique():\n",
    "            filtered_df = df[df['source'] == source]\n",
    "            index_points = filtered_df[filtered_df['index'] == index_val]\n",
    "\n",
    "            # Add scatter points\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=index_points['x'],\n",
    "                y=index_points['Cramers\\' V'],\n",
    "                mode='markers+text',\n",
    "                text=index_points['Cramers\\' V'].round(3),\n",
    "                textposition='top center',\n",
    "                marker=dict(size=10, color=color_map[source]),\n",
    "                hoverinfo='text',\n",
    "                showlegend=False  # No legend for individual traces\n",
    "            ), row=row, col=col)\n",
    "\n",
    "            # Add lines if there are multiple points\n",
    "            if len(index_points) > 1:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=index_points['x'],\n",
    "                    y=index_points['Cramers\\' V'],\n",
    "                    mode='lines',\n",
    "                    line=dict(width=2, color=color_map[source]),\n",
    "                    showlegend=False\n",
    "                ), row=row, col=col)\n",
    "\n",
    "        fig.add_annotation(\n",
    "            text=index_val,\n",
    "            xref='paper', yref='paper',\n",
    "            x=(col - 1) / num_cols + 1 / (2 * num_cols),  # Center in the column\n",
    "            y= 0.5-((row - 1) / num_rows - 0.05),  # Adjusted for better placement\n",
    "            showarrow=False,\n",
    "            font=dict(size=12)  # Adjust font size if needed\n",
    "        )\n",
    "\n",
    "    # Customizing the layout\n",
    "    fig.update_layout(\n",
    "        title_text='Cramers V for Prompt Features',\n",
    "        title_x=0.5,  # Center the title\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white'\n",
    "    )\n",
    "\n",
    "    # Hide x-axis labels\n",
    "    for row in range(1, num_rows + 1):\n",
    "        for col in range(1, num_cols + 1):\n",
    "            fig.update_xaxes(showticklabels=False, row=row, col=col)\n",
    "\n",
    "    # Set y-axis title for each subplot\n",
    "    for row in range(1, num_rows + 1):\n",
    "        fig.update_yaxes(title_text='Cramer\\'s V', row=row, col=1)\n",
    "\n",
    "    fig.write_html('cramer_waveExperiment_2x3.html')\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "fig=visualize_cramer_waveExperiment_2x3(cramer_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ablationExperiment_pmf():\n",
    "    wave_number=12 # ablation is only performed on #12\n",
    "    demographics= get_wave_demographics(wave_number)\n",
    "    survey_labels = get_demographics_and_labels(wave_number,demographics)\n",
    "    llm_labels_dict={}\n",
    "    \n",
    "    llm_population_pmf={}\n",
    "    llm_group_pmf={}\n",
    "    for experiment in ablation_experiments:\n",
    "        llm_labels = get_demographics_and_llm_labels(wave_number,experiment,demographics)\n",
    "        llm_labels_dict[experiment]=llm_labels\n",
    "        llm_population_pmf[experiment] =     calculate_pmf_population(llm_labels)\n",
    "        llm_group_pmf[experiment] =     calculate_pmf_by_groups(llm_labels)\n",
    "\n",
    "        \n",
    "    survey_population_pmf =  calculate_pmf_population(survey_labels)\n",
    "    survey_group_pmf =  calculate_pmf_by_groups(survey_labels)\n",
    "    survey_population_df=pd.DataFrame(survey_population_pmf)\n",
    "    return survey_population_df,survey_group_pmf,llm_population_pmf,llm_group_pmf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "def get_JS_group_plot_waveExperiment(survey_population_df, fname='JS_group_plot.html'):\n",
    "    unique_groups = survey_population_df['social_group'].unique()\n",
    "    colors = px.colors.qualitative.Set2\n",
    "    group_colors = {category: colors[i % len(colors)] for i, category in enumerate(unique_groups)}\n",
    "\n",
    "    survey_population_df = survey_population_df.sort_values(by=['wave', 'social_group_category'])\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Get unique categories in the order they appear on the x-axis\n",
    "    categories = survey_population_df['social_group_category'].unique()\n",
    "\n",
    "    for category in categories:\n",
    "        subset = survey_population_df[survey_population_df['social_group_category'] == category]\n",
    "        social_groups = subset['social_group'].unique()\n",
    "\n",
    "        for social_group in social_groups:\n",
    "            group_subset = subset[subset['social_group'] == social_group]\n",
    "\n",
    "            # Split the data into two parts: below and above 0.6\n",
    "            below_threshold = group_subset[group_subset['js'] <= 0.6]\n",
    "            above_threshold = group_subset[group_subset['js'] > 0.6]\n",
    "\n",
    "            # Add the main line (below or equal to 0.6)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[group_subset['social_group_category'], group_subset['wave']],\n",
    "                y=group_subset['js'].clip(upper=0.6),\n",
    "                mode='lines+markers',\n",
    "                marker=dict(color=group_colors[social_group]),\n",
    "                line=dict(color=group_colors[social_group]),\n",
    "                text=group_subset['social_group'],\n",
    "                name=social_group,\n",
    "                legendgroup=category,\n",
    "                legendgrouptitle_text=category,\n",
    "            ))\n",
    "\n",
    "            if not above_threshold.empty:\n",
    "                # Find the first occurrence of a point above threshold\n",
    "                for i in range(0, above_threshold.shape[0]):\n",
    "                    point = above_threshold.iloc[i]\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=[[point['social_group_category']], [point['wave']]],\n",
    "                        y=[0.6],  # Set y to 0.6 for the point\n",
    "                        mode='markers+text',\n",
    "                        marker=dict(color=group_colors[social_group], symbol='triangle-up', size=13),\n",
    "                        textposition='top center',\n",
    "                        name=social_group,\n",
    "                        showlegend=False,\n",
    "                    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='JS Distance between Survey Answers and LLM Answers Over Time with Social Group Categories',\n",
    "        xaxis_title='Time Index (Wave)',\n",
    "        yaxis_title='JS Values',\n",
    "        yaxis_range=[survey_population_df['js'].min() - 0.02, 0.62],\n",
    "        margin=dict(b=100, t=100, r=50),\n",
    "        legend=dict(\n",
    "            orientation='h',\n",
    "            yanchor='bottom',\n",
    "            y=-0.6,  # Adjust this value to add more vertical space\n",
    "            xanchor='center',\n",
    "            x=0.5,\n",
    "            title='',\n",
    "            groupclick='toggleitem',\n",
    "            itemsizing='constant',\n",
    "            traceorder='grouped'\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add annotation for values exceeding 0.6\n",
    "    fig.add_annotation(\n",
    "        xref='paper', yref='paper',\n",
    "        x=1.02, y=1,\n",
    "        text=\"▲ Values > 0.6\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=10),\n",
    "        align='left',\n",
    "        xanchor='left',\n",
    "        yanchor='top'\n",
    "    )\n",
    "\n",
    "    fig.write_html(fname)\n",
    "\n",
    "# Example usage\n",
    "# group_JS should be defined here for this to work, the DataFrame should match the structure used in the function\n",
    "# get_JS_group_plot_waveExperiment(group_JS, fname='JS_group_plot2.html')\n",
    "\n",
    "    \n",
    "get_JS_group_plot_waveExperiment(group_JS,fname='JS_group_plot2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_waveExperiment_population_comparison(df1, df2, title1='Survey Population', title2='LLM Population', output_file='waveExperiment_population_comparison.html'):\n",
    "    \"\"\"\n",
    "    compare categorey percentages over time \n",
    "    \"\"\"\n",
    "    # Transpose DataFrames to get timestamps as rows and categories as columns\n",
    "    df1 = df1.T\n",
    "    df2 = df2.T\n",
    "\n",
    "    # Convert DataFrame index to wave dates\n",
    "    df1.index = df1.index.map(lambda x: wave_dates.get(x, x))\n",
    "    df2.index = df2.index.map(lambda x: wave_dates.get(x, x))\n",
    "    # Determine all unique categories from both datasets\n",
    "    all_categories = sorted(set(df1.columns).union(set(df2.columns)))\n",
    "\n",
    "    # Sort categories by the highest value in df1 in descending order\n",
    "    sorted_categories = sorted(all_categories, key=lambda category: df1[category].max() if category in df1.columns else float('-inf'), reverse=True)\n",
    "\n",
    "    # Create subplots with 1 row for each category and 1 column\n",
    "    fig = make_subplots(rows=len(sorted_categories), cols=1, shared_xaxes=True, vertical_spacing=0.02,\n",
    "                        subplot_titles=[f\"{i+1}. {category}\" for i, category in enumerate(sorted_categories)])\n",
    "\n",
    "    # Add traces for df1 and df2 for each category\n",
    "    for i, category in enumerate(sorted_categories):\n",
    "        if category in df1.columns:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df1.index,\n",
    "                y=df1[category] * 100,  # Convert to percentage\n",
    "                mode='lines+markers',\n",
    "                name=f'{title1} - {category}',\n",
    "                hovertemplate='%{y:.2f}%',\n",
    "                showlegend=False \n",
    "            ), row=i + 1, col=1)\n",
    "\n",
    "        if category in df2.columns:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df2.index,\n",
    "                y=df2[category] * 100,  # Convert to percentage\n",
    "                mode='lines+markers',\n",
    "                name=f'{title2} - {category}',\n",
    "                hovertemplate='%{y:.2f}%',  # Only show percentage\n",
    "                showlegend=False\n",
    "            ), row=i + 1, col=1)\n",
    "\n",
    "    ticktexts=[f\"wave {k}:<br> {v}\" for k, v in wave_dates.items()]\n",
    "    for i in range(1, len(sorted_categories) + 1):\n",
    "        fig.update_yaxes(title_text=\"Percentage (%)\", tickformat=\".2f\", row=i, col=1)\n",
    "        fig.update_xaxes(title_text=\"Survey Wave\",tickvals=list(wave_dates.values()), ticktext=ticktexts, row=i, col=1)\n",
    "        fig.layout[f'xaxis{i}']['showticklabels']=True\n",
    "        \n",
    "    # Update overall layout\n",
    "    fig.update_layout(\n",
    "        height=300 * len(sorted_categories),\n",
    "        margin=dict(l=50, r=50, t=50, b=50),\n",
    "        showlegend=False,\n",
    "        #xaxis_showticklabels=True,\n",
    "    )\n",
    "\n",
    "    # Save the plot as an HTML file\n",
    "    fig.write_html(output_file, auto_open=True)\n",
    "\n",
    "    # Return the plot figure\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "fig=plot_waveExperiment_population_comparison(survey_population_df,llm_population_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ideas to plot entropy\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "df= group_level_entropy_results.groupby(['social_group','source'])['entropy'].mean().reset_index()\n",
    "\n",
    "# Pivot the DataFrame\n",
    "df_pivot = df.pivot(index='social_group', columns='source', values='entropy').reset_index()\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.scatter(df_pivot, x='survey', y='llm', text='social_group', \n",
    "                 title='Survey vs LLM Entropy by Social Group',\n",
    "                 labels={'survey': 'Survey Entropy', 'llm': 'LLM Entropy'})\n",
    "\n",
    "# Add the y=x line\n",
    "fig.add_trace(go.Scatter(x=[min(df_pivot['survey']), max(df_pivot['survey'])], \n",
    "                         y=[min(df_pivot['survey']), max(df_pivot['survey'])], \n",
    "                         mode='lines', \n",
    "                         name='y=x', \n",
    "                         line=dict(color='Red', dash='dash')))\n",
    "\n",
    "# Add social group labels to each point\n",
    "fig.update_traces(textposition='top center')\n",
    "\n",
    "# Show plot\n",
    "fig.write_html('entropy.html')\n",
    "\n",
    "# entropy of the survey on the x-axis\n",
    "# js distance on the y \n",
    "# color: social_class_category\n",
    "# dots: social_class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def proportional_agreement(df1, df2):\n",
    "#     # Ensure both dataframes have the same columns\n",
    "#     if set(df1.columns) != set(df2.columns):\n",
    "#         raise ValueError(\"Dataframes must have the same columns\")\n",
    "    \n",
    "#     # Ensure both dataframes have the same number of rows\n",
    "#     if len(df1) != len(df2):\n",
    "#         raise ValueError(\"Dataframes must have the same number of rows\")\n",
    "    \n",
    "#     agreement = {}\n",
    "#     for col in df1.columns:\n",
    "#         # Calculate the number of agreements for each column\n",
    "#         agree = (df1[col] == df2[col]).sum()\n",
    "#         # Calculate the proportional agreement\n",
    "#         prop_agr = agree / len(df1)\n",
    "#         agreement[col] = prop_agr\n",
    "    \n",
    "#     return agreement\n",
    "# proportional_agreement(survey_labels_dict[12]['highest_prob_label'],llm_labels_dict[12][labels_16])\n",
    "\n",
    "# def proportional_agreement_from_freq(df1, df2):\n",
    "#     # Ensure both dataframes have the same columns\n",
    "#     if set(df1.columns) != set(df2.columns):\n",
    "#         raise ValueError(\"Dataframes must have the same columns\")\n",
    "    \n",
    "#     agreement = {}\n",
    "#     for col in df1.columns:\n",
    "#         # Sum the minimum counts for each category in the frequency tables\n",
    "#         min_counts = (df1[col] + df2[col]) - abs(df1[col] - df2[col])\n",
    "#         total_counts = df1[col] + df2[col]\n",
    "        \n",
    "#         # Calculate the proportional agreement\n",
    "#         prop_agr = min_counts.sum() / total_counts.sum()\n",
    "#         agreement[col] = prop_agr\n",
    "    \n",
    "#     return agreement\n",
    "\n",
    "\n",
    "# proportional_agreement_from_freq(survey_group_pmf[12].T, llm_group_pmf[12].T)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
