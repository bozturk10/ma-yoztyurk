{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src\n",
    "import numpy as np\n",
    "from src.paths import MODELS_DIR\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from src.data.process_data import process_open_ended, process_wave_data,process_open_ended\n",
    "from src.data.read_data import load_raw_survey_data, read_stata_file\n",
    "from src.paths import CODING_DIR, GLES_DIR, PROCESSED_DATA_DIR, ANNOTATED_GENERATIONS_DIR,RAW_DATA_DIR\n",
    "from src.utils import get_lang\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from src.paths import RESULTS_DIR\n",
    "classid2trainid = {int(classname):idx  for idx, classname in enumerate(sorted(pd.read_csv(os.path.join(CODING_DIR,'map.csv')).upperclass_id.unique())) }\n",
    "df_lookup= pd.read_csv(os.path.join(CODING_DIR,'map.csv'))\n",
    "label2str= dict(zip(df_lookup.upperclass_id,df_lookup.upperclass_name))\n",
    "label_names= [label2str[i] for i in range(0,len(label2str)) ]\n",
    "\n",
    "labels_16= [label_name for label_name in label_names if label_name!='LLM refusal']\n",
    "label_names= [label2str[i] for i in range(0,len(label2str)) ]\n",
    "labels_14= [label_name for label_name in label_names if label_name not in ['LLM refusal' ,'keine Angabe','wei√ü nich'] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from src.analysis.data_processing import get_demographics_and_labels, get_demographics_and_llm_labels, get_wave_demographics\n",
    "from src.analysis.experiment_utils import get_JS_experiment, get_MI_experiment, get_experiment_entropy\n",
    "from src.analysis.metrics import calculate_pmf_by_groups, calculate_pmf_population, get_entropy_JS_corr_data,get_cramerV, get_cramerV_multiclass, get_population_level_ape_results\n",
    "from src.analysis.waveExperiment.utils import   get_waveExperiment_data\n",
    "from src.analysis.data_processing import social_group_to_category,social_category_to_group\n",
    "from src.analysis.waveExperiment.utils import   get_waveExperiment_data\n",
    "\n",
    "from src.analysis.waveExperiment.plots import *\n",
    "\n",
    "from src.analysis.modelExperiment.utils import get_modelExperiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Experiment 1,2,3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_JS1_multilabel, group_JS1_multilabel = get_JS_experiment(\n",
    "    survey_population_df_multilabel1, llm_population_df_multilabel1, survey_group_pmf_multilabel1, llm_group_pmf_multilabel1\n",
    ")\n",
    "\n",
    "population_JS1_multiclass, group_JS1_multiclass = get_JS_experiment(\n",
    "    survey_population_df_multiclass1, llm_population_df_multiclass1, survey_group_pmf_multiclass1, llm_group_pmf_multiclass1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_dates={\n",
    "10: '06-11-2018',\n",
    "11: '28-05-2019',\n",
    "12: '05-11-2019',\n",
    " 13: '21-04-2020',\n",
    " 14: '03-11-2020',\n",
    " 15: '25-02-2021',\n",
    " 16: '06-05-2021',\n",
    " 17: '07-07-2021',\n",
    " 18: '11-08-2021',\n",
    " 19: '15-09-2021',\n",
    " 20: '29-09-2021',\n",
    " 21: '09-12-2021'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.highest_prob_label=plot_df.highest_prob_label.map(coarse_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare lists to hold data for plotting\n",
    "plot_data = []\n",
    "\n",
    "for key, df in survey_labels_dict2.items():\n",
    "    # Get value counts for the current DataFrame\n",
    "    counts = df['highest_prob_label'].value_counts(1).reset_index()\n",
    "    counts.columns = ['Answer Labels', 'freq']  # Renamed column\n",
    "    \n",
    "    # Add a column for time\n",
    "    counts['time'] = wave_dates[key]\n",
    "    \n",
    "    # Append to list\n",
    "    plot_data.append(counts)\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "plot_df = pd.concat(plot_data, ignore_index=True)\n",
    "plot_df['Answer Labels'] = plot_df['Answer Labels'].map(coarse_translation)  # Updated column name\n",
    "\n",
    "# Calculate the average frequency for each label\n",
    "avg_freq = plot_df.groupby('Answer Labels')['freq'].mean().reset_index()\n",
    "avg_freq = avg_freq.sort_values(by='freq', ascending=False)\n",
    "\n",
    "# Get the top 5 labels\n",
    "top_5_lines = avg_freq.head(5)['Answer Labels']\n",
    "\n",
    "# Choose a distinct color palette\n",
    "color_palette = px.colors.qualitative.Plotly\n",
    "\n",
    "# Plotting\n",
    "fig = px.line(plot_df, x='time', y='freq', color='Answer Labels', markers=True,\n",
    "              labels={'freq': 'Number of Occurrences', 'time': 'Date', 'Answer Labels': 'Answer Labels'},\n",
    "             color_discrete_sequence=color_palette)\n",
    "\n",
    "fig.update_layout(xaxis_title='Date', yaxis_title='Frequency')\n",
    "\n",
    "# Remove background\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white'\n",
    ")\n",
    "\n",
    "# Add text labels\n",
    "fig.update_traces(text=plot_df['freq'], textposition='top center')\n",
    "\n",
    "# Add annotations with arrows for the peak points of the top 5 lines\n",
    "annotations = []\n",
    "for i, label in enumerate(top_5_lines):\n",
    "    line_data = plot_df[plot_df['Answer Labels'] == label]\n",
    "    # Use the peak data point for the annotation\n",
    "    peak_point = line_data.loc[line_data['freq'].idxmax()]\n",
    "    x_val = peak_point['time']\n",
    "    y_val = peak_point['freq']\n",
    "    annotations.append(dict(\n",
    "        x=x_val,\n",
    "        y=y_val,\n",
    "        text=f\"{label}\",\n",
    "        showarrow=True,\n",
    "        arrowhead=2,\n",
    "        ax=120,  # Position arrow and text further to the right\n",
    "        ay=-30 - i * 20,  # Adjust vertical offset to avoid overlap\n",
    "        font=dict(color='black')\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Survey Start Date',\n",
    "    yaxis_title='Frequency',\n",
    "    title_x=0.5,\n",
    "    font=dict(size=15),\n",
    "    \n",
    "    # Set dimensions to fit A4 size with more width\n",
    "    width=12 * 80,  # Increased width (12 inches)\n",
    "    height=11.69 * 80,  # Height remains the same (11.69 inches)\n",
    "    \n",
    "    # Position the legend horizontally below the plot\n",
    "    legend=dict(\n",
    "        orientation='h',\n",
    "        yanchor='top',\n",
    "        y=-0.2,  # Adjust the y position if needed\n",
    "        xanchor='center',\n",
    "        x=0.5\n",
    "    ),\n",
    "    \n",
    "    # Add annotations\n",
    "    annotations=annotations\n",
    ")\n",
    "\n",
    "fig.write_html('aaab.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MI_results_waveExperiment = get_MI_experiment(survey_labels_dict, llm_labels_dict)\n",
    "\n",
    "population_JS, group_JS = get_JS_experiment(\n",
    "   survey_population_df_multilabel3,  \n",
    "llm_population_df_multilabel3, \n",
    "survey_group_pmf_multilabel3,  \n",
    "llm_group_pmf_multilabel3,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cramer_results3_mc = get_cramerV_multiclass(survey_labels_dict3, llm_labels_dict3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_level_entropy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.ablationExperiment.utils import ablation_mapped_dict\n",
    "get_ablation_cramer_table2(cramer_results3_mc,save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ablation_cramer_table2(cramer_ablation_df,save=False,fname='cramer_table.csv'):\n",
    "\n",
    "    cramer_ablation_df= cramer_ablation_df.pivot(columns=['source'],values=['Cramers\\' V'],index=['index','wave_id']).reset_index()\n",
    "    def filter_index(row):\n",
    "        a=row['index'][0]\n",
    "        exp_values= ablation_mapped_dict[a]\n",
    "        wave_id=row['wave_id'][0]\n",
    "        #print(a,wave_id,exp_values)\n",
    "        if (wave_id in exp_values) and (wave_id!='Llama2_base') and ('without_' not in wave_id) :\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_experiment_type(row):\n",
    "        a=row['wave_id'][0]\n",
    "        if '1VAR' in a:\n",
    "            return 'one variable'\n",
    "        elif 'all' in a:\n",
    "            return 'all variables'\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    cramer_ablation_df['filter']=cramer_ablation_df.apply(filter_index,axis=1)     \n",
    "    cramer_ablation_df=cramer_ablation_df[cramer_ablation_df['filter']==True].sort_values(by='index')        \n",
    "    cramer_ablation_df['exp_type']=cramer_ablation_df.apply(get_experiment_type ,axis=1)\n",
    "    cramer_ablation_df['Cramers\\' V']=cramer_ablation_df['Cramers\\' V'].round(3)\n",
    "    cramer_ablation_df2=cramer_ablation_df.reset_index(drop=True).drop(['wave_id','filter'],axis=1).reset_index()\n",
    "    cramer_ablation_df2_one=cramer_ablation_df2[cramer_ablation_df2['exp_type']=='one variable']\n",
    "    cramer_ablation_df2_all=cramer_ablation_df2[cramer_ablation_df2['exp_type']=='all variables']#.query(\"exp_type=='one variable' \")\n",
    "\n",
    "    c=pd.merge(cramer_ablation_df2_all,cramer_ablation_df2_one,on='index')\n",
    "    c.columns = [' '.join(col).strip() for col in c.columns.values]\n",
    "    return c\n",
    "    c.columns=['level_0_x', 'prompt variable', 'Cramers\\' V (all variables)', 'Cramers\\' V (survey)',\n",
    "           'exp_type_x', 'level_0_y', 'Cramers\\' V (one variable)', 'Cramers\\' V (survey)',\n",
    "           'exp_type_y']\n",
    "    c=c[[ 'prompt variable', 'Cramers\\' V (survey)','Cramers\\' V (all variables)','Cramers\\' V (one variable)']]\n",
    "    if save==True:\n",
    "        c.to_csv(os.path.join(RESULTS_DIR,'ablationExperiment',fname))\n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.metrics import (\n",
    "\n",
    "    get_js_dist_by_groups,\n",
    ")\n",
    "ablation_js= get_js_dist_population(survey_population_df, llm_population_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in llm_labels_dict3.keys():\n",
    "    a=llm_labels_dict3[k]['text_llm'].apply(is_about_covid).value_counts(1).min()\n",
    "    print(k,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_dict = {#for plotting\n",
    "    '18-29 YEARS': '18-29',\n",
    "    '30-44 YEARS': '30-44',\n",
    "    '45-59 YEARS': '45-59',\n",
    "    '60 and more': '60+',\n",
    "    'Die Linke': 'Linke',\n",
    "    'Ostdeutschland': 'Ostdeutschland',\n",
    "    'Westdeutschland': 'Westdeutschland',\n",
    "    'befindet sich noch in beruflicher Ausbildung.': 'beruflicher Ausbildung',\n",
    "    'die AfD': 'AfD',\n",
    "    'die CDU/CSU': 'CDU/CSU',\n",
    "    'die FDP': 'FDP',\n",
    "    'die Gr√ºnen': 'Gr√ºnen',\n",
    "    'die SPD': 'SPD',\n",
    "    'eine Kleinpartei': 'Kleinpartei',\n",
    "    'hat das Abitur': 'Abitur',\n",
    "    'hat ein Berufliches Praktikum oder Volontariat abgeschlossen.': 'Berufliches Praktikum/Volontariat',\n",
    "    'hat eine Lehre abgeschlossen.': 'Lehre abgeschlossen',\n",
    "    'hat eine gewerbliche oder landwirtschaftliche Lehre abgeschlossen.': 'gewerbliche/landwirtschaftliche Lehre',\n",
    "    'hat eine kaufm√§nnische Lehre abgeschlossen.': 'kaufm√§nnische Lehre',\n",
    "    'hat einen Berufsfachschulabschluss.': 'Berufsfachschulabschluss',\n",
    "    'hat einen Fachhochschulabschluss.': 'Fachhochschulabschluss',\n",
    "    'hat einen Fachhochschulreife': 'Fachhochschulreife',\n",
    "    'hat einen Fachschulabschluss.': 'Fachschulabschluss',\n",
    "    'hat einen Hauptschulabschluss': 'Hauptschulabschluss',\n",
    "    'hat einen Meisterabschluss oder Technikerabschluss.': 'Meister-/Technikerabschluss',\n",
    "    'hat einen Realschulabschluss': 'Realschulabschluss',\n",
    "    'hat einen Universit√§tsabschluss.': 'Universit√§tsabschluss',\n",
    "    'hat keine berufliche Ausbildung abgeschlossen.': 'keine berufliche Ausbildung',\n",
    "    'hat keinen Schulabschluss': 'kein Schulabschluss',\n",
    "    'keine Partei': 'keine Partei',\n",
    "    'm√§nnlich': 'm√§nnlich',\n",
    "    'weiblich': 'weiblich'\n",
    "}\n",
    "coarse_translation = {\n",
    "    \"Politische Strukturen und Prozesse\": \"Political System and Processes\",\n",
    "    \"Sozialpolitik\": \"Social Policy\",\n",
    "    \"Gesundheitspolitik\": \"Health Policy\",\n",
    "    \"Familien- und Gleichstellungspolitik\": \"Family and Gender Equality Policy\",\n",
    "    \"Bildungspolitik\": \"Education Policy\",\n",
    "    \"Umweltpolitik\": \"Environmental  Policy\",\n",
    "    \"Wirtschaftspolitik\": \"Economic  Policy\",\n",
    "    \"Sicherheits\": \"Security\",\n",
    "    \"Au√üenpolitik\": \"Foreign  Policy\",\n",
    "    \"Medien und Kommunikation\": \"Media and  Communication\",\n",
    "    \"Sonstiges\": \"Others\",\n",
    "    \"Migration und Integration\": \"Migration and  Integration\",\n",
    "    \"Ostdeutschland\": \"East  Germany\",\n",
    "    \"keine Angabe\": \"Not  specified\",\n",
    "    \"wei√ü nich\": \"Do not know\",\n",
    "    \"LLM refusal\": \"LLM refusal\",\n",
    "    \"Werte, politische Kultur und Gesellschaftskritik\": \"Values, political culture and general  social criticism\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_level_entropy_results_multiclass, group_level_entropy_results_multiclass = (\n",
    "    get_experiment_entropy(\n",
    "        survey_population_df_multiclass3, llm_population_df_multiclass3, survey_group_pmf_multiclass3, llm_group_pmf_multiclass3\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_level_entropy_results_multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_JS_mc2, group_JS_mc2 = get_JS_experiment(\n",
    "    survey_population_df_multiclass2, llm_population_df_multiclass2, survey_group_pmf_multiclass2, llm_group_pmf_multiclass2\n",
    ")\n",
    "population_level_entropy_results_mc2, group_level_entropy_results_mc2 = (\n",
    "    get_experiment_entropy(\n",
    "        survey_population_df_multiclass2, llm_population_df_multiclass2, survey_group_pmf_multiclass2, llm_group_pmf_multiclass2\n",
    "    )\n",
    ")\n",
    "\n",
    "population_JS_ml2, group_JS_ml2 = get_JS_experiment(\n",
    "        survey_population_df_multilabel2, llm_population_df_multilabel2, survey_group_pmf_multilabel2, llm_group_pmf_multilabel2\n",
    ")\n",
    "population_level_entropy_results_ml2, group_level_entropy_results_ml2 = (\n",
    "    get_experiment_entropy(\n",
    "        survey_population_df_multilabel2, llm_population_df_multilabel2, survey_group_pmf_multilabel2, llm_group_pmf_multilabel2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_level_entropy_results_ml2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# population_JS_mc3, group_JS_mc3 = get_JS_experiment(\n",
    "#     survey_population_df_multiclass3, llm_population_df_multiclass3, survey_group_pmf_multiclass3, llm_group_pmf_multiclass3\n",
    "# )\n",
    "# population_level_entropy_results_mc3, group_level_entropy_results_mc3 = (\n",
    "#     get_experiment_entropy(\n",
    "#         survey_population_df_multiclass3, llm_population_df_multiclass3, survey_group_pmf_multiclass3, llm_group_pmf_multiclass3\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# population_JS_ml3, group_JS_ml3 = get_JS_experiment(\n",
    "#         survey_population_df_multilabel3, llm_population_df_multilabel3, survey_group_pmf_multilabel3, llm_group_pmf_multilabel3\n",
    "# )\n",
    "# population_level_entropy_results_ml3, group_level_entropy_results_ml3 = (\n",
    "#     get_experiment_entropy(\n",
    "#         survey_population_df_multilabel3, llm_population_df_multilabel3, survey_group_pmf_multilabel3, llm_group_pmf_multilabel3\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.metrics import calculate_pmf_by_groups, calculate_pmf_population, get_entropy_JS_corr_data,get_cramerV, get_cramerV_multiclass, get_population_level_ape_results,get_entropy_JS_corr_data_no_mean\n",
    "\n",
    "entropy_JS_corr_data_ml2=get_entropy_JS_corr_data(group_JS_ml2,group_level_entropy_results_ml2)\n",
    "entropy_JS_corr_data_mc2=get_entropy_JS_corr_data(group_JS_mc2,group_level_entropy_results_mc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=pd.merge(group_JS_ml2,group_level_entropy_results_ml2.query(\"source=='survey'\"),left_on=['wave','social_group'],right_on=['wave_id','social_group']).query(\"social_group_category=='leaning_party'\")\n",
    "k\n",
    "for val in k.wave_id.unique():\n",
    "    print('v',val)\n",
    "    print(k.query(f\"wave_id=={val}\")[['js','entropy']].corr())\n",
    "    print('===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val='leaning_party'\n",
    "a= entropy_JS_corr_data_ml2.query(f\"social_group_category=='{val}'\")#.plot.scatter(x='entropy',y='js',c='text')#[['js','entropy']]#.corr()#.iloc[1,0]\n",
    "a.text=a.text.astype('category')\n",
    "a.plot.scatter(x='entropy',y='js',c='text',cmap='tab10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in entropy_JS_corr_data_mc2.social_group_category.unique():\n",
    "    print('v',val)\n",
    "    print(entropy_JS_corr_data_mc2.query(f\"social_group_category=='{val}'\")[['js','entropy']].corr())\n",
    "    print('===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaning_party 0.49\n",
    "berufabschluss_clause -0.16\n",
    "schulabschluss_clause 0.73\n",
    "age_groups 0.48\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_JS_corr_data_mc2.query(\"social_group_category=='leaning_party'\")[['js','entropy']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_JS_corr_data_mc2.query(\"social_group_category=='leaning_party'\")[['js','entropy']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=population_level_entropy_results_mc2.pivot(index=['study'],columns=['source'],values=['shannon_entropy']).T\n",
    "a=pd.concat([a, population_JS_mc2], axis=0)\n",
    "a=a.T\n",
    "print(a.columns)\n",
    "a.columns=['entropy_llm','entropy_survey','js']\n",
    "a['diff']=a['entropy_survey']-a['entropy_llm']\n",
    "a['absdiff']=a['diff'].abs()\n",
    "a.T.round(3).astype(str).to_latex('ex2_population_js_entropy_multiclass.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.plot.scatter(x='entropy_survey',y='js')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.plot.scatter(x='entropy_llm',y='js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=population_level_entropy_results_ml2.pivot(index=['study'],columns=['source'],values=['shannon_entropy']).T\n",
    "a=pd.concat([a, population_JS_ml2], axis=0)\n",
    "a=a.T\n",
    "a.columns=['entropy_llm','entropy_survey','js']\n",
    "a['diff']=a['entropy_survey']-a['entropy_llm']\n",
    "a['absdiff']=a['diff'].abs()\n",
    "a.T.round(3).astype(str).to_latex('ex2_population_js_entropy_multilabel.txt')\n",
    "a.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #mi =get_MI_from_dataset( survey_labels_dict3['Llama2_all'] )\n",
    "# #mi\n",
    "# mi =get_MI_from_dataset( llm_labels_dict3['Llama2_all'] )\n",
    "# mi =get_MI_from_dataset( llm_labels_dict3['1VAR_party'] )\n",
    "population_JS1_multilabel, group_JS1_multilabel = get_JS_experiment(\n",
    "    survey_population_df_multilabel1, llm_population_df_multilabel1, survey_group_pmf_multilabel1, llm_group_pmf_multilabel1\n",
    ")\n",
    "\n",
    "population_JS1_multiclass, group_JS1_multiclass = get_JS_experiment(\n",
    "    survey_population_df_multiclass1, llm_population_df_multiclass1, survey_group_pmf_multiclass1, llm_group_pmf_multiclass1\n",
    ")\n",
    "group_JS1_multiclass[\"social_group\"]=group_JS1_multiclass[\"social_group\"].replace(shortened_dict)\n",
    "\n",
    "\n",
    "group_JS1_multilabel[\"social_group\"]=group_JS1_multilabel[\"social_group\"].replace(shortened_dict)\n",
    "\n",
    "\n",
    "population_JS1_multilabel['social_group_category']='population'\n",
    "population_JS1_multilabel['social_group']='population'\n",
    "\n",
    "population_JS1_multiclass['social_group_category']='population'\n",
    "population_JS1_multiclass['social_group']='population'\n",
    "ex1_JS_latex_ml=pd.concat([population_JS1_multilabel.set_index(['social_group_category','social_group']),group_JS1_multilabel.pivot(index=['social_group_category','social_group'],values='js',columns=['wave']) ])\n",
    "ex1_JS_latex_ml.columns= ex1_JS_latex_ml.columns.map({'google-gemma-7b-it_12_1712704376_modified':'gemma', 'Llama2_all':'Llama2',\n",
    "       'mistralai-Mixtral-8x7B-Instruct-v0.1_12_1712772173':'mixtral'})\n",
    "ex1_JS_latex_ml=ex1_JS_latex_ml.round(3).astype(str)\n",
    "ex1_JS_latex_ml.to_latex('ex1_JS_multilabel.txt',\n",
    "        index=True,\n",
    "        escape=False,\n",
    "        sparsify=True,\n",
    "        multirow=True,\n",
    "        multicolumn=True,\n",
    "        multicolumn_format='c',\n",
    "        position='p',\n",
    "        bold_rows=True\n",
    "    )\n",
    "ex1_JS_latex_mc=pd.concat([population_JS1_multiclass.set_index(['social_group_category','social_group']),group_JS1_multiclass.pivot(index=['social_group_category','social_group'],values='js',columns=['wave']) ])\n",
    "ex1_JS_latex_mc.columns= ex1_JS_latex_mc.columns.map({'google-gemma-7b-it_12_1712704376_modified':'gemma', 'Llama2_all':'Llama2',\n",
    "       'mistralai-Mixtral-8x7B-Instruct-v0.1_12_1712772173':'mixtral'})\n",
    "ex1_JS_latex_mc=ex1_JS_latex_mc.round(3).astype(str)\n",
    "\n",
    "ex1_JS_latex_mc.to_latex('ex1_JS_multiclass.txt',\n",
    "        index=True,\n",
    "        escape=False,\n",
    "        sparsify=True,\n",
    "        multirow=True,\n",
    "        multicolumn=True,\n",
    "        multicolumn_format='c',\n",
    "        position='p',\n",
    "        bold_rows=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l(group_level_entropy_results,population_level_entropy_results,exp_to_filter=['1VAR_party','Llama2_base','Llama2_all'],var='leaning_party'):\n",
    "\n",
    "    group_level_entropy_results['social_group_category']=group_level_entropy_results['social_group'].map(social_group_to_category)#.groupby('source')['entropy'].describe()#.query(\"study=='Llama2_all'\")#.head()\n",
    "    #group_level_entropy_results['weighted_entropy']=group_level_entropy_results['social_group'].map(d)* group_level_entropy_results['entropy']#.map(d)#.groupby('source')['entropy'].describe()#.query(\"study=='Llama2_all'\")#.head()\n",
    "    k=group_level_entropy_results[group_level_entropy_results['wave_id'].isin(exp_to_filter) & group_level_entropy_results['social_group_category'].isin([var]) ]\n",
    "\n",
    "\n",
    "    pop_entropy=population_level_entropy_results#.pivot(index='study',values='shannon_entropy',columns='source').reset_index()\n",
    "    #pop_entropy['social_group_category']='population'\n",
    "    #pop_entropy['social_group']='population'\n",
    "    pop_entropy=pop_entropy.rename({'study':'wave_id','shannon_entropy':'entropy_population'},axis=1)\n",
    "    pop_entropy=pop_entropy[pop_entropy['wave_id'].isin(exp_to_filter) ]\n",
    "\n",
    "    l= pd.merge(k,pop_entropy,on=['wave_id','source'],suffixes=('_sg','_pop')  )\n",
    "    #row_mask= l[l['source']=='survey']\n",
    "    return l \n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "def get_info_gain_plot(l,fname='entropy_info_gain.html'):\n",
    "    # Create a subplot figure with 2 rows and 4 columns\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=3, \n",
    "        subplot_titles=l['social_group'].unique(),\n",
    "        specs=[ [{\"secondary_y\": True}]*3, [{\"secondary_y\": True}]*3,[{\"secondary_y\": True}]*3,[{\"secondary_y\": True}]*3],\n",
    "           horizontal_spacing=0.20,  # Increase horizontal spacing\n",
    "        vertical_spacing=0.15  # Reduce vertical spacing\n",
    "    )\n",
    "\n",
    "    row = 1\n",
    "    col = 1\n",
    "\n",
    "    colors = px.colors.qualitative.Plotly  # You can choose any color sequence\n",
    "\n",
    "    # Store seen wave_ids to avoid legend duplication\n",
    "    seen_wave_ids = set()\n",
    "\n",
    "    # Loop through each unique social group\n",
    "    for i, k in enumerate(l['social_group'].unique()):\n",
    "        df_long = l[l['social_group'] == k].melt(id_vars=['wave_id'], value_vars=['entropy', 'entropy_population'],\n",
    "                                                 var_name='type', value_name='value')\n",
    "        df_long['type'] = pd.Categorical(df_long['type'], categories=['entropy_population', 'entropy'], ordered=True)\n",
    "        df_long = df_long.sort_values('type')\n",
    "\n",
    "        # Create line plots for each type of entropy\n",
    "        for j, wave_id in enumerate(df_long['wave_id'].unique()):\n",
    "            df_wave = df_long[df_long['wave_id'] == wave_id]\n",
    "\n",
    "            # Assign a consistent color for both lines and markers\n",
    "            color = colors[j % len(colors)]  # Cycle through the color palette\n",
    "\n",
    "            # Add primary y-axis trace for 'entropy_population' type\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_wave['type'], \n",
    "                    y=df_wave['value'], \n",
    "                    mode='lines+markers', \n",
    "                    name=f'{wave_id} - Entropy', \n",
    "                    line=dict(color=color),\n",
    "                    marker=dict(color=color),\n",
    "                    legendgroup=f'Wave {wave_id}',\n",
    "                    showlegend=(wave_id not in seen_wave_ids)  # Show legend only once for each wave_id\n",
    "                ),\n",
    "                row=row, col=col, secondary_y=False\n",
    "            )\n",
    "\n",
    "            # Add secondary y-axis trace for 'entropy' type\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_wave['type'], \n",
    "                    y=df_wave['value'], \n",
    "                    mode='lines+markers', \n",
    "                    name=f'{wave_id} - Entropy', \n",
    "                    line=dict(dash='dash', color=color),\n",
    "                    marker=dict(color=color),\n",
    "                    legendgroup=f'{wave_id}',\n",
    "                    showlegend=False  # Hide legend for this entry\n",
    "                ),\n",
    "                row=row, col=col, secondary_y=True\n",
    "            )\n",
    "\n",
    "            # Mark the wave_id as seen\n",
    "            seen_wave_ids.add(wave_id)\n",
    "\n",
    "        # Update row and column for next subplot\n",
    "        col += 1\n",
    "        if col > 3:\n",
    "            col = 1\n",
    "            row += 1\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=1000, width=1200, \n",
    "        legend_title_text='Model', \n",
    "        legend=dict(\n",
    "            orientation='h',\n",
    "            yanchor='bottom',\n",
    "            y=-0.4,  # Adjust this value to move the legend closer to the plot\n",
    "            xanchor='center',\n",
    "            x=0.5,\n",
    "            itemsizing='constant'  # Ensures consistent item sizing in the legend\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Update axis labels for all subplots\n",
    "    for i in range(1, len(l['social_group'].unique()) ):  # Assuming there are 8 subplots\n",
    "        fig.update_xaxes(title_text=\"Type\", row=(i-1)//3+1, col=(i-1)%3+1)\n",
    "        fig.update_yaxes(title_text=\"Pop. Entropy\", row=(i-1)//3+1, col=(i-1)%3+1)\n",
    "        fig.update_yaxes(title_text=\"Subpop. Entropy\", secondary_y=True, row=(i-1)//3+1, col=(i-1)%3+1)\n",
    "\n",
    "    # Save the plot to an HTML file\n",
    "    fig.write_html(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def get_survey_to_survey_JS_distances(survey_population_df, fname='s2s_JS_dist.html'):\n",
    "    rs = []\n",
    "    \n",
    "    for wave_id in [17, 18, 19, 20, 21]:\n",
    "        for col in survey_population_df.columns:\n",
    "            if col <= wave_id:\n",
    "                js = distance.jensenshannon(survey_population_df[col], survey_population_df[wave_id])\n",
    "                r = {\n",
    "                    'd1': col,\n",
    "                    'd2': wave_id,\n",
    "                    'js': js\n",
    "                }\n",
    "                rs.append(r)\n",
    "    \n",
    "    df = pd.DataFrame(rs)\n",
    "\n",
    "    df['text'] = \"wave \" + df['d1'].astype(str) + \"<br>\" + df['d1'].map(wave_dates)\n",
    "    df['d2'] = df['d2'].astype(str)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add a trace for each category in d2\n",
    "    for category in df['d2'].unique():\n",
    "        category_df = df[df['d2'] == category]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=category_df['d1'],\n",
    "            y=category_df['js'],\n",
    "            mode='lines+markers',\n",
    "            name=str(category),\n",
    "            text=category_df['text'],\n",
    "        ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Wave - Date',\n",
    "        yaxis_title='JS',\n",
    "        xaxis_tickvals=df['d1'],\n",
    "        xaxis_ticktext=df['text'],\n",
    "        yaxis=dict(\n",
    "            tickmode='linear',\n",
    "            tick0=0,\n",
    "            dtick=0.05,\n",
    "            showgrid=True,\n",
    "            gridcolor='rgba(128, 128, 128, 0.5)',\n",
    "            gridwidth=1,\n",
    "            griddash='dash',  # Make the grid lines dashed\n",
    "            zeroline=True,\n",
    "            zerolinecolor='rgba(128, 128, 128, 0.5)',\n",
    "            zerolinewidth=1,\n",
    "        ),\n",
    "        plot_bgcolor='white',\n",
    "        legend_font=dict(size=20),\n",
    "        font=dict(size=20),\n",
    "    )\n",
    "\n",
    "    # Save to HTML\n",
    "    fig.write_html(fname)\n",
    "    return fig\n",
    "#get_survey_to_survey_JS_distances(survey_population_df_multilabel2,fname='ex2_s2s_JS_dist_multilabel.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_level_entropy_results1_multilabel, group_level_entropy_results1_multilabel = (\n",
    "    get_experiment_entropy(\n",
    "        survey_population_df_multilabel1, llm_population_df_multilabel1, survey_group_pmf_multilabel1, llm_group_pmf_multilabel1\n",
    "    )\n",
    ")\n",
    "\n",
    "population_level_entropy_results1_multiclass, group_level_entropy_results1_multiclass = (\n",
    "    get_experiment_entropy(\n",
    "        survey_population_df_multiclass1, llm_population_df_multiclass1, survey_group_pmf_multiclass1, llm_group_pmf_multiclass1\n",
    "    )\n",
    ")\n",
    "\n",
    "l = get_l(group_level_entropy_results1_multilabel,population_level_entropy_results1_multilabel,   exp_to_filter=['mistralai-Mixtral-8x7B-Instruct-v0.1_12_1712772173','Llama2_all','Llama3_70B_all'])\n",
    "l.loc[l['source']=='survey','wave_id']='survey'\n",
    "l=l.drop_duplicates(subset=['wave_id','social_group','source'])\n",
    "l.wave_id=l.wave_id.map({'survey':'survey','Llama2_all':'Llama2',\n",
    "       'mistralai-Mixtral-8x7B-Instruct-v0.1_12_1712772173':'Mixtral'})\n",
    "#get_info_gain_plot(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.analysis.metrics import  get_entropy_JS_corr_data,get_cramerV, get_cramerV_multiclass, get_population_level_ape_results\n",
    "\n",
    "# ape_results1= get_population_level_ape_results(survey_population_df_multilabel3,llm_population_df_multilabel3,survey_group_pmf_multilabel3,llm_group_pmf_multilabel3,save=True,experiment_type='ablationExperiment',file_name='ape_results_multilabel.csv')\n",
    "# ape_results2= get_population_level_ape_results(survey_population_df_multiclass3,llm_population_df_multiclass3,survey_group_pmf_multiclass3,llm_group_pmf_multiclass3,save=True,experiment_type='ablationExperiment',file_name='ape_results_multilabel.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= (llm_population_df_multiclass3- survey_population_df_multiclass3)*100 #.sum()*100\n",
    "df['1VAR']= df.filter(like='1VAR').min(axis=1)\n",
    "df['without']= df.filter(like='without_').min(axis=1)\n",
    "df=df[['Llama2_base','1VAR','without','Llama2_all']].round(2)\n",
    "mask= df.abs().eq( (df.abs().min(axis=1)) ,axis=0)\n",
    "df.applymap( lambda x :  f\"\\\\textbf{{{round(x,2)}}}\").where(mask,df).astype(str).to_latex('ex3_PE_by_category.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_level_entropy_results3_multilabel, group_level_entropy_results3_multilabel = (\n",
    "    get_experiment_entropy(\n",
    "        survey_population_df_multilabel3, llm_population_df_multilabel3, survey_group_pmf_multilabel3, llm_group_pmf_multilabel3\n",
    "    )\n",
    ")\n",
    "\n",
    "population_level_entropy_results3_multiclass, group_level_entropy_results3_multiclass = (\n",
    "    get_experiment_entropy(\n",
    "        survey_population_df_multilabel3, llm_population_df_multilabel3, survey_group_pmf_multilabel3, llm_group_pmf_multilabel3\n",
    "    )\n",
    ")\n",
    "\n",
    "l = get_l(group_level_entropy_results3_multilabel,population_level_entropy_results3_multilabel,   exp_to_filter=['1VAR_age','Llama2_base','Llama2_all'], var='age_groups')\n",
    "\n",
    "l.loc[l['source']=='survey','wave_id']='survey'\n",
    "l=l.drop_duplicates(subset=['wave_id','social_group','source'])\n",
    "\n",
    "# l.wave_id=l.wave_id.replace({'survey':'survey','Llama2_all':'Llama2',\n",
    "#        'mistralai-Mixtral-8x7B-Instruct-v0.1_12_1712772173':'Mixtral'})\n",
    "get_info_gain_plot(l,'ex3_age.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_level_entropy_results3_multilabel.social_group_category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_level_entropy_results3_multilabel.wave_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=group_level_entropy_results.pivot(index=['wave_id','social_group'],values='entropy',columns=['source']).reset_index()\n",
    "# a['social_group_category']=a['social_group'].map(social_group_to_category)\n",
    "# a.query(\"social_group_category=='leaning_party'\").head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_level_entropy_results3_multilabel.social_group_category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "d_list=[]\n",
    "for k in survey_labels_dict2.keys():\n",
    "    df= survey_labels_dict2[k].filter(regex='gender|^age_groups$|clause|party|ostwest|eastwest|highest_prob_label')\n",
    "    df=sample_random_label_from_strata(df)\n",
    "    \n",
    "    d= {'wave':k,\n",
    "     'acc_survey':accuracy_score(df['highest_prob_label'], df['new_sampled_label']),\n",
    "     'kappa_survey':cohen_kappa_score(df['highest_prob_label'], df['new_sampled_label'])\n",
    "    }\n",
    "    \n",
    "    df= llm_labels_dict2[k].filter(regex='gender|^age_groups$|clause|party|ostwest|eastwest|highest_prob_label|highest_prob_label_llm')\n",
    "    #df=sample_random_label_from_strata(df)\n",
    "    d.update(\n",
    "    {\n",
    "    'acc_llm': accuracy_score(df['highest_prob_label'], df['highest_prob_label_llm']),\n",
    "    'kappa_llm':cohen_kappa_score(df['highest_prob_label'], df['highest_prob_label_llm'])\n",
    "    }\n",
    "    )\n",
    "    d_list.append(d)\n",
    "    print('==================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(d_list)\n",
    "df.set_index('wave', inplace=True)\n",
    "df=df.T\n",
    "df.round(2).astype(str).to_latex('a.txt')#.to_csv('sampledSurvey_and_llm_acc_kappa_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_percentage_table(survey_population_df_multilabel2,llm_population_df_multilabel2):\n",
    "\n",
    "    def color_cell(value, threshold=0):\n",
    "        value=float(value)\n",
    "        if value > 1:\n",
    "            color = 'ForestGreen'\n",
    "            return f\"\\\\textcolor{{{color}}}{{value}}\"\n",
    "        elif value < -1 :\n",
    "            color='red'\n",
    "            return f\"\\\\textcolor{{{color}}}{{value}}\"\n",
    "        else:\n",
    "            color='black'\n",
    "            return f\"\\\\textcolor{{{color}}}{{value}}\"\n",
    "\n",
    "\n",
    "    a= survey_population_df_multilabel2.multiply(100).round(1)#.astype(str)\n",
    "    b= llm_population_df_multilabel2.multiply(100).round(1)#.astype(str)\n",
    "    colordf=(b-a).applymap(color_cell)\n",
    "    for col in colordf.columns:\n",
    "        colordf[col]=colordf[col].combine(b[col],lambda fmt,value: fmt.replace('value',str(value)))\n",
    "    b=colordf#.combine(b[12],lambda fmt,value: fmt.format(value))\n",
    "    \n",
    "    a['src']='survey'\n",
    "    b['src']='llm'\n",
    "    a=a.astype(str)\n",
    "    b.loc[:,'mean APE']= ( llm_population_df_multilabel2-survey_population_df_multilabel2).divide(survey_population_df_multilabel2).multiply(100).abs().mean(axis=1).round(2).astype(str)\n",
    "    #b.loc[:,'mean PE']= ( llm_population_df_multilabel2-survey_population_df_multilabel2).divide(survey_population_df_multilabel2).multiply(100).mean(axis=1)\n",
    "\n",
    "    c=pd.concat([a.set_index([a.index,'src']),b.set_index([b.index,'src'])]).sort_index()\n",
    "    c['mean APE']= c['mean APE'].fillna('')\n",
    "    #c['mean PE']= c['mean PE'].fillna('')\n",
    "\n",
    "    c.loc['APE',:]=np.append(( llm_population_df_multilabel2-survey_population_df_multilabel2).round(2).multiply(100).abs().sum(axis=0).values , [''])\n",
    "    return c #c#.to_latex('a.txt')\n",
    "a=get_labels_percentage_table(survey_population_df_multilabel1.drop('Llama3_70B_all',axis=1),llm_population_df_multilabel1.drop('Llama3_70B_all',axis=1) )\n",
    "a.to_latex('bb.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "for k in llm_labels_dict2.keys():\n",
    "    df= llm_labels_dict2[k].filter(regex='gender|^age_groups$|clause|party|ostwest|eastwest|highest_prob_label')\n",
    "    print(k,accuracy_score(df['highest_prob_label'], df['highest_prob_label_llm']))\n",
    "    print(k,cohen_kappa_score(df['highest_prob_label'], df['highest_prob_label_llm']))\n",
    "    print('==================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def is_about_covid(text):\n",
    "    pattern = r'\\b(covid|corona|coronavirus|covid[-\\s]?19|sars[-\\s]?cov[-\\s]?2)\\b'\n",
    "    return re.search(pattern, text, re.IGNORECASE) is not None\n",
    "# for k in llm_labels_dict1.keys():\n",
    "#     survey_labels_dict1[k]['text_covid']=survey_labels_dict1[k]['text'].apply(is_about_covid)#.value_counts(1)\n",
    "#     print(survey_labels_dict1[k]['text_covid'].value_counts(1))\n",
    "for k in llm_labels_dict1.keys():\n",
    "    llm_labels_dict1[k]['text_covid']=llm_labels_dict1[k]['text_llm'].apply(is_about_covid)#.value_counts(1)\n",
    "    print(k,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in llm_labels_dict1.keys():\n",
    "    print(k,llm_labels_dict1[k]['text_llm'].sample(30).values,'\\n')\n",
    "\n",
    "#introductory sentence percentages\n",
    "for k in llm_labels_dict1.keys():\n",
    "    #print(k,llm_labels_dict1[k]['text_llm'].sample(10).values,'\\n')\n",
    "    if 'gemma' in k:\n",
    "        print(k,llm_labels_dict1[k]['text_llm'].str[:3].value_counts(1),'\\n')\n",
    "        print(k,llm_labels_dict1[k]['text_llm'].str.startswith('Als Deutschin mit deutscher Staatsb√ºrgerschaft').value_counts(1),'\\n')\n",
    "    elif 'Llama' in k:\n",
    "        print(k,llm_labels_dict1[k]['text_llm'].str[:10].value_counts(1),'\\n')\n",
    "        print(k,llm_labels_dict1[k]['text_llm'].str.startswith('Das wichtigste Problem').value_counts(1),'\\n')\n",
    "    elif 'mistralai' in k :\n",
    "        print(k,llm_labels_dict1[k]['text_llm'].str[:10].value_counts(1),'\\n')\n",
    "        print(k,llm_labels_dict1[k]['text_llm'].str.startswith('Eines der wichtigsten Probleme').value_counts(1),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.ablationExperiment.plot import get_ablation_JS_plot\n",
    "from src.analysis.experiment_utils import get_JS_experiment, get_experiment_entropy\n",
    "from src.analysis.metrics import get_cramerV, get_population_level_ape_results,get_cramerV_multiclass,get_population_level_ape_results\n",
    "from src.analysis.modelExperiment.plot import get_modelExperiment_pmf_comparison\n",
    "from src.analysis.modelExperiment.utils import get_modelExperiment_data, get_textual_stats\n",
    "from src.analysis.data_processing import labels_16\n",
    "population_JS, group_JS = get_JS_experiment(\n",
    "    survey_population_df_multilabel1, llm_population_df_multilabel1, survey_group_pmf_multilabel1, llm_group_pmf_multilabel1\n",
    ")\n",
    "population_level_entropy_results, group_level_entropy_results = (\n",
    "    get_experiment_entropy(\n",
    "        survey_population_df_multilabel1, llm_population_df_multilabel1, survey_group_pmf_multilabel1, llm_group_pmf_multilabel1\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "js_population_fig= get_ablation_JS_plot(population_JS,save=True,fname='js_population_fig_multilabel.html')\n",
    "js_population_fig.write_html('js_population_fig_multilabel.html')\n",
    "\n",
    "population_JS.to_csv('population_JS_multilabel.csv')\n",
    "population_level_entropy_results.to_csv('population_level_entropy_results_multilabel.csv')\n",
    "population_JS, group_JS = get_JS_experiment(\n",
    "    survey_population_df_multiclass1, llm_population_df_multiclass1, survey_group_pmf_multiclass1, llm_group_pmf_multiclass1\n",
    ")\n",
    "population_JS.to_csv('population_JS_multiclass.csv')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_colnames_nonzero(row):\n",
    "    return '_'.join([col for col in row.index if row[col] != 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_textual_stats(llm_labels_dict1,survey_labels_dict1):\n",
    "    d={}\n",
    "    llm_labels_dict1['survey'] = survey_labels_dict1[list(survey_labels_dict1.keys())[0]] #any key from survey_labels_dict1 will work, they are repeetitions just to match llm_labels_dict\n",
    "\n",
    "    for k,df in llm_labels_dict1.items():\n",
    "        avg_label_cnt=df[labels_16].sum(axis=1).mean()\n",
    "        avg_sample_per_label= df[labels_16].sum(axis=0).mean()\n",
    "        if k =='survey':\n",
    "            avg_word_count=df['text'].apply(lambda x: len(x.split())).mean()\n",
    "        else:\n",
    "            avg_word_count=df['text_llm'].apply(lambda x: len(x.split())).mean()\n",
    "        labels_concatted= df[labels_16].apply(concat_colnames_nonzero,axis=1)\n",
    "        labels_concatted= labels_concatted[labels_concatted.str.contains(\"_\")]\n",
    "        lbl_vc= labels_concatted.value_counts(1).head(5)\n",
    "        d[k]={\n",
    "            'avg_label_cnt':avg_label_cnt,\n",
    "            'avg_sample_per_label':avg_sample_per_label,\n",
    "            'avg_word_count':avg_word_count,\n",
    "        }\n",
    "    df= pd.DataFrame(d).round(2)#.to_csv()\n",
    "    return df \n",
    "get_textual_stats(llm_labels_dict1,survey_labels_dict1).to_csv('textual_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_modelExperiment_pmf_comparison(llm_population_df,survey_population_df,fname='pmf_comparison_1.html',save=False):\n",
    "#     df = llm_population_df.copy()\n",
    "#     df['wave 12']=survey_population_df.iloc[:,0]\n",
    "#     df.columns=['gemma-7b-it', 'lama-2-13b-chat-hf',\n",
    "#            'mistralai-Mixtral-8x7B-Instruct', 'wave 12']\n",
    "#     df= df.apply(lambda x: (x*100).round(1) ) #['wave 12']\n",
    "#     coarse_translation = {\n",
    "#         \"Politische Strukturen und Prozesse\": \"Political System <br> and Processes\",\n",
    "#         \"Sozialpolitik\": \"Social <br> Policy\",\n",
    "#         \"Gesundheitspolitik\": \"Health <br> Policy\",\n",
    "#         \"Familien- und Gleichstellungspolitik\": \"Family and <br> Gender Equality <br> Policy\",\n",
    "#         \"Bildungspolitik\": \"Education <br> Policy\",\n",
    "#         \"Umweltpolitik\": \"Environmental <br> Policy\",\n",
    "#         \"Wirtschaftspolitik\": \"Economic <br> Policy\",\n",
    "#         \"Sicherheits\": \"Security\",\n",
    "#         \"Au√üenpolitik\": \"Foreign <br> Policy\",\n",
    "#         \"Medien und Kommunikation\": \"Media and <br> Communication\",\n",
    "#         \"Sonstiges\": \"Others\",\n",
    "#         \"Migration und Integration\": \"Migration and <br> Integration\",\n",
    "#         \"Ostdeutschland\": \"East <br> Germany\",\n",
    "#         \"keine Angabe\": \"Not <br> specified\",\n",
    "#         \"wei√ü nich\": \"Do not know\",\n",
    "#         \"LLM refusal\": \"LLM refusal\",\n",
    "#         \"Werte, politische Kultur und Gesellschaftskritik\": \"Values,<br> political culture<br> and general <br> social criticism\"\n",
    "#     }\n",
    "\n",
    "#     df.index=df.index.map(coarse_translation)\n",
    "#     import plotly.graph_objects as go\n",
    "\n",
    "#     def plot_comparison_chart(llm_population_df, title, output_file_path, save=False, width=1):\n",
    "#         figs = []\n",
    "#         for col in llm_population_df.columns:\n",
    "#             data = llm_population_df[col]\n",
    "#             figs.append(go.Bar(\n",
    "#                 x=data.index, \n",
    "#                 y=data.values, \n",
    "#                 name=col,\n",
    "#                 width=width,\n",
    "#                  text=data.values,         \n",
    "#                 textposition='outside', )\n",
    "#             )\n",
    "\n",
    "#         # Combine the traces in a single figure\n",
    "#         fig = go.Figure(data=figs)\n",
    "\n",
    "#         fig.update_layout(\n",
    "#             title=title,\n",
    "#             yaxis_title='Percentage',\n",
    "#             barmode='group',  \n",
    "#             bargap=0.30,  \n",
    "#             bargroupgap=0.35,  \n",
    "#             legend=dict(\n",
    "#                 orientation='h',\n",
    "#                 yanchor='bottom',\n",
    "#                 y=1.02,\n",
    "#                 xanchor='right',\n",
    "#                 x=1,\n",
    "#             ),     \n",
    "#             font=dict(size=15),\n",
    "#             plot_bgcolor='rgba(0, 0, 0, 0)',\n",
    "#         )\n",
    "\n",
    "#         def split_label(label):\n",
    "#             if isinstance(label, str) and len(label) > 10:\n",
    "#                 middle = len(label) // 2\n",
    "#                 space_pos = label.rfind(' ', 0, middle)\n",
    "#                 comma_pos = label.rfind(',', 0, middle)\n",
    "\n",
    "#                 split_pos = max(space_pos, comma_pos) if max(space_pos, comma_pos) != -1 else middle\n",
    "\n",
    "#                 return f\"{label[:split_pos+1]}<br>{label[split_pos+1:]}\"\n",
    "#             return label\n",
    "\n",
    "#         tickvals = llm_population_df.index\n",
    "#         ticktext = [split_label(x) for x in tickvals]\n",
    "\n",
    "#         # Generate positions for separator lines based on the index of categorical values\n",
    "#         separator_positions = [i + 0.5 for i in range(len(tickvals) - 1)]\n",
    "\n",
    "#         fig.update_xaxes(\n",
    "#             ticktext=ticktext,\n",
    "#             tickvals=tickvals,\n",
    "#             tickfont=dict(color='black'),\n",
    "#             tickangle=0,  # Set tick angle to 0 to make text horizontal\n",
    "#             showgrid=False,\n",
    "#             ticks='outside',\n",
    "#             ticklen=10,  # Length of the ticks\n",
    "#             tickwidth=2,  # Width of the ticks\n",
    "#             tickcolor='white'\n",
    "#         )\n",
    "\n",
    "#         fig.update_yaxes(tickfont=dict(color='black'))\n",
    "\n",
    "#         # Add separator lines using shapes\n",
    "#         shapes = []\n",
    "#         for pos in separator_positions:\n",
    "#             shapes.append(dict(\n",
    "#                 type='line',\n",
    "#                 x0=pos,\n",
    "#                 x1=pos,\n",
    "#                 y0=0,\n",
    "#                 y1=-0.025,\n",
    "#                 xref='x',\n",
    "#                 yref='paper',\n",
    "#                 line=dict(color='black', width=2)\n",
    "#             ))\n",
    "\n",
    "#         fig.update_layout(shapes=shapes)\n",
    "\n",
    "#         if save and output_file_path.endswith(\".html\"):    \n",
    "#             # Save the figure as an HTML file\n",
    "#             fig.write_html(output_file_path)\n",
    "#         elif save and output_file_path.endswith(\".png\"):\n",
    "#             fig.write_image(output_file_path)\n",
    "#         else:\n",
    "#             fig.write_image(output_file_path, engine=\"kaleido\")\n",
    "\n",
    "#         return fig\n",
    "#     fig=plot_comparison_chart(llm_population_df=df, title = \"\", output_file_path= 'pmf_comparison_1.html', save=True, width=0.2)\n",
    "#     return fig\n",
    "# get_modelExperiment_pmf_comparison(survey_population_df=survey_population_df_multilabel1,llm_population_df= llm_population_df_multilabel1, fname= 'pmf_comparison_1.html', save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import itertools\n",
    "# # Assuming df_train is your DataFrame containing the training data\n",
    "\n",
    "# # Splitting the data\n",
    "# X_train = df_train.drop(columns=['highest_prob_label'])\n",
    "# Y_train = df_train['highest_prob_label'].values\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Encoding the target variable Y_train\n",
    "# le = LabelEncoder()\n",
    "# y_train_encoded = le.fit_transform(y_train)\n",
    "\n",
    "# # Define a function to convert encoded labels back to original values\n",
    "# def decode_label(encoded_labels):\n",
    "#     return le.inverse_transform(encoded_labels)\n",
    "\n",
    "# # Initializing XGBClassifier\n",
    "# #clf = XGBClassifier(tree_method=\"hist\", enable_categorical=True)\n",
    "\n",
    "\n",
    "# vc=(1/pd.Series(y_train_encoded).value_counts(1)) \n",
    "# dict(zip(vc.keys(),vc.values))\n",
    "\n",
    "# clf = XGBClassifier(tree_method=\"hist\", enable_categorical=True, scale_pos_weight=class_weights)\n",
    "\n",
    "\n",
    "\n",
    "# # Training the classifier\n",
    "# clf.fit(X_train, y_train_encoded)\n",
    "\n",
    "# # Predicting on validation set\n",
    "# y_val_pred_encoded = clf.predict(X_val)\n",
    "\n",
    "# # Decoding predicted labels back to original values\n",
    "# y_val_pred = decode_label(y_val_pred_encoded)\n",
    "\n",
    "# # Print accuracy and classification report\n",
    "# print(\"Accuracy on validation set:\", accuracy_score(y_val, y_val_pred))\n",
    "# print(\"Classification report:\")\n",
    "# print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# conf_mat = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# # Plot confusion matrix\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.imshow(conf_mat, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.colorbar()\n",
    "# tick_marks = np.arange(len(le.classes_))\n",
    "# plt.xticks(tick_marks, le.classes_, rotation=45)\n",
    "# plt.yticks(tick_marks, le.classes_)\n",
    "# plt.xlabel('Predicted Label')\n",
    "# plt.ylabel('True Label')\n",
    "\n",
    "# # Print numbers inside the plot\n",
    "# thresh = conf_mat.max() / 2.\n",
    "# for i, j in itertools.product(range(conf_mat.shape[0]), range(conf_mat.shape[1])):\n",
    "#     plt.text(j, i, format(conf_mat[i, j], 'd'),\n",
    "#              horizontalalignment=\"center\",\n",
    "#              color=\"white\" if conf_mat[i, j] > thresh else \"black\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "# import lightgbm as lgb\n",
    "\n",
    "# # Assuming df_train and df_test are already defined and preprocessed\n",
    "\n",
    "# # Separate features and labels\n",
    "# X_train = df_train.drop(columns=['highest_prob_label'])\n",
    "# Y_train = df_train['highest_prob_label']\n",
    "\n",
    "# # Ensure the test set is correctly assigned\n",
    "# X_test = df_test.drop(columns=['highest_prob_label'])\n",
    "# Y_test = df_test['highest_prob_label']\n",
    "\n",
    "# # Encode target labels if they are categorical\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train = label_encoder.fit_transform(Y_train)\n",
    "# y_test = label_encoder.transform(Y_test)\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Initialize LightGBM classifier\n",
    "# clf = lgb.LGBMClassifier(boosting_type='gbdt', objective='multiclass', n_jobs=-1, random_state=42,class_weight ='balanced')\n",
    "\n",
    "# # Fit the classifier\n",
    "# clf.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "\n",
    "# # Predict and evaluate on validation set\n",
    "# y_val_pred = clf.predict(X_val)\n",
    "# print(classification_report(y_val, y_val_pred))\n",
    "# print(f\"Validation Accuracy: {accuracy_score(y_val, y_val_pred)}\")\n",
    "\n",
    "# # Predict and evaluate on test set\n",
    "# y_test_pred = clf.predict(X_test)\n",
    "# print(classification_report(y_test, y_test_pred))\n",
    "# print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "# df=df.sample(frac=0.2)\n",
    "# #df=survey_labels_dict[12].filter(regex='gender|^age_groups$|clause|party|ostwest|eastwest|highest_prob_label')\n",
    "# X = df[['ostwest', 'leaning_party', 'gender','age_groups','schulabschluss_clause','berufabschluss_clause']]\n",
    "# preds=[]\n",
    "# Y = df['highest_prob_label'].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# # One-hot encoding for categorical variables\n",
    "# encoder = OneHotEncoder(drop='first',sparse_output=False)\n",
    "# encodery = OneHotEncoder(drop='first',sparse_output=False)\n",
    "\n",
    "# X_encoded = encoder.fit_transform(X)\n",
    "# y_encoded = encodery.fit_transform(Y)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# #exog = sm.add_constant(X_train)\n",
    "# X_train_exog = sm.add_constant(X_train)\n",
    "\n",
    "# model = sm.MNLogit(y_train, X_train_exog)\n",
    "# result = model.fit_regularized()\n",
    "\n",
    "# print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_encoded_exog = sm.add_constant(X_encoded)\n",
    "# res=result.predict(X_encoded_exog)#>0.5\n",
    "# p=pd.Series(np.argmax(res,axis=1)).value_counts(1).sort_index()\n",
    "# q=pd.Series(np.argmax(y_encoded,axis=1)).value_counts(1).sort_index()\n",
    "# from scipy.spatial import distance\n",
    "# distance.jensenshannon(p,q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes classifier \n",
    "import numpy as np\n",
    "rng = np.random.RandomState(1)\n",
    "df=survey_labels_dict[16].filter(regex='gender|^age_groups$|clause|party|ostwest|eastwest|highest_prob_label')\n",
    "\n",
    "#df=survey_labels_dict[12].filter(regex='gender|^age_groups$|clause|party|ostwest|eastwest|highest_prob_label')\n",
    "X = df[['ostwest', 'leaning_party', 'gender','age_groups','schulabschluss_clause','berufabschluss_clause']]\n",
    "preds=[]\n",
    "Y = df['highest_prob_label'].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "encoder = OneHotEncoder(drop='first',sparse_output=False)\n",
    "\n",
    "\n",
    "\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "y_encoded = encodery.fit_transform(Y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "res= clf.predict(X_test)\n",
    "q=pd.Series( df['highest_prob_label']).value_counts(1)\n",
    "p=pd.Series(res).value_counts(1).reindex(q.index).fillna(0)\n",
    "distance.jensenshannon(p,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=result.predict(X_encoded_exog)#>0.5\n",
    "p=pd.Series(np.argmax(res,axis=1)).value_counts(1).sort_index()\n",
    "q=pd.Series(np.argmax(y_encoded,axis=1)).value_counts(1).sort_index()\n",
    "from scipy.spatial import distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=result.predict(X_encoded_exog)#>0.5\n",
    "p=pd.Series(np.argmax(res,axis=1)).value_counts(1).sort_index()\n",
    "q=pd.Series(np.argmax(y_encoded,axis=1)).value_counts(1).sort_index()\n",
    "from scipy.spatial import distance\n",
    "distance.jensenshannon(p,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off scientific notation in Pandas and Numpy\n",
    "#pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "entropy(p,q,base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_exog = sm.add_constant(X_encoded)\n",
    "preds_label= model.predict(X_exog).tolist()\n",
    "preds.append(preds_label)\n",
    "#poisson_model.predict(X_test)\n",
    "print('===========================================')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "X = df[['ostwest', 'leaning_party', 'gender', 'age_groups']]\n",
    "preds=[]\n",
    "for lbl in labels_16:\n",
    "    Y = df[lbl].values\n",
    "\n",
    "\n",
    "    # One-hot encoding for categorical variables\n",
    "    encoder = OneHotEncoder(drop='first',sparse_output=False)\n",
    "    X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "    exog = sm.add_constant(X_train)\n",
    "\n",
    "    # Fit a Poisson regression model\n",
    "    poisson_model = sm.GLM(y_train, exog, family=sm.families.NegativeBinomial()).fit()\n",
    "    print('label',lbl)\n",
    "    # Summary of the model\n",
    "    #print(poisson_model.summary())\n",
    "    X_exog = sm.add_constant(X_encoded)\n",
    "    preds_label= poisson_model.predict(X_exog).tolist()\n",
    "    preds.append(preds_label)\n",
    "    #poisson_model.predict(X_test)\n",
    "    print('===========================================')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[labels_16].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = poisson_model.predict(X_test)\n",
    "\n",
    "# For evaluation, you can calculate metrics like Mean Squared Error (MSE) or compare the predicted counts with actual counts.\n",
    "mse = np.mean((y_test - y_pred) ** 2)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,y_train.shape,X_test.shape,y_test.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.waveExperiment.utils import get_JS_waveExperiment\n",
    "\n",
    "\n",
    "population_JS, group_JS = get_JS_waveExperiment(\n",
    "    survey_population_df, llm_population_df, survey_group_pmf, llm_group_pmf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.process_data import process_open_ended, process_wave_data,process_open_ended\n",
    "wave_dates=[]\n",
    "for i in range(10,22):\n",
    "    wave_number=i\n",
    "    wave_df, wave_open_ended_df, df_coding_840s = load_raw_survey_data(wave_number)\n",
    "    field_end=wave_df.field_end.unique()\n",
    "    print(field_end)\n",
    "    wave_dates.append(field_end[0])\n",
    "#wave_open_ended_df = process_open_ended(wave_open_ended_df, df_coding_840s, wave_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coding_840s.filter(regex=f'kp{13}_840_c1', axis=1).kp13_840_c1.value_counts(1,dropna=True).head(60)#.kp12_840s.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lingua import Language, LanguageDetectorBuilder\n",
    "languages = [Language.ENGLISH,  Language.GERMAN]\n",
    "\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "def get_langs(text):\n",
    "    det_langs=[]\n",
    "    for result in detector.detect_multiple_languages_of(text):\n",
    "        #print(f\"{result.language.name}: '{sentence[result.start_index:result.end_index]}'\")\n",
    "        det_langs.append(result.language.name)\n",
    "    return '_'.join(sorted(list(set(det_langs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_labels_dict['Llama2_all']#['text_llm_lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= llm_labels_dict['Llama2_all'][['text_llm','text_llm_lang']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text_llm_lang.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"text_llm_lang=='ENGLISH_GERMAN'\").text_llm.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lingua-language-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.metrics import (\n",
    "    calculate_cramerV,\n",
    "    calculate_group_entropy,\n",
    "    calculate_pmf_by_groups,\n",
    "    calculate_pmf_population,\n",
    "    calculate_population_entropy,\n",
    "    get_MI_from_dataset,\n",
    "    calculate_cramerV_multiclass,\n",
    "    get_js_dist_by_groups,\n",
    "    get_js_dist_population,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_js_dist_population(survey_population_df,llm_population_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_js_dist_population(df1,df2):\n",
    "    if type(df1)==pd.DataFrame:\n",
    "        assert (df1.columns==df2.columns).all()\n",
    "    \n",
    "        js_arr=distance.jensenshannon(df1,df2).tolist()\n",
    "        df= pd.DataFrame([js_arr],columns=df1.columns)\n",
    "    return  df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "survey_population_df2=survey_population_df[~survey_population_df.index.str.contains('keine Angabe|wei√ü nich|Medien und Kommunikation')]\n",
    "llm_population_df2=llm_population_df[~llm_population_df.index.str.contains('keine Angabe|wei√ü nich|Medien und Kommunikation')]\n",
    "\n",
    "entropy(survey_population_df2,llm_population_df2,base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_population_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bert.utils import get_experiment_df\n",
    "from src.paths import RESULTS_DIR\n",
    "from tqdm import tqdm\n",
    "\n",
    "classid2trainid = {int(classname):idx  for idx, classname in enumerate(sorted(pd.read_csv(os.path.join(CODING_DIR,'map.csv')).upperclass_id.unique())) }\n",
    "df_lookup= pd.read_csv(os.path.join(CODING_DIR,'map.csv'))\n",
    "label2str= dict(zip(df_lookup.upperclass_id,df_lookup.upperclass_name))\n",
    "label_names= [label2str[i] for i in range(0,len(label2str)) ]\n",
    "labels_16= [label_name for label_name in label_names if label_name!='LLM refusal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bert.utils import get_experiment_df\n",
    "from src.paths import RESULTS_DIR\n",
    "from tqdm import tqdm\n",
    "\n",
    "classid2trainid = {int(classname):idx  for idx, classname in enumerate(sorted(pd.read_csv(os.path.join(CODING_DIR,'map.csv')).upperclass_id.unique())) }\n",
    "df_lookup= pd.read_csv(os.path.join(CODING_DIR,'map.csv'))\n",
    "label2str= dict(zip(df_lookup.upperclass_id,df_lookup.upperclass_name))\n",
    "label_names= [label2str[i] for i in range(0,len(label2str)) ]\n",
    "labels_16= [label_name for label_name in label_names if label_name!='LLM refusal']\n",
    "ablation_experiments= ['1VAR_age',\n",
    " '1VAR_berufabschluss',\n",
    " '1VAR_eastwest',\n",
    " '1VAR_gender',\n",
    " '1VAR_party',\n",
    " '1VAR_schulabschluss',\n",
    " 'Llama2_all',\n",
    " 'Llama2_base',\n",
    " 'without_age',\n",
    " 'without_berufabschluss',\n",
    " 'without_eastwest',\n",
    " 'without_gender',\n",
    " 'without_party',\n",
    " 'without_schulabschluss']\n",
    "\n",
    "\n",
    "model_comparison_experiments= [\n",
    " 'google-gemma-7b-it_12_1712704376_modified',\n",
    " 'Llama2_all',\n",
    " 'mistralai-Mixtral-8x7B-Instruct-v0.1_12_1712772173'\n",
    "]\n",
    "\n",
    "wave_experiments= ['12/Llama2_all',\n",
    " '13/Llama2_all',\n",
    " '14/Llama2_all',\n",
    " '15/Llama2_all',\n",
    " '16/Llama2_all',\n",
    " '17/Llama2_all',\n",
    " '18/Llama2_all',\n",
    " '19/Llama2_all',\n",
    " '20/Llama2_all',\n",
    " '21/Llama2_all']\n",
    "\n",
    "social_groups=[ 'ostwest','berufabschluss_clause', 'leaning_party', 'gender','schulabschluss_clause', 'age_groups']\n",
    "\n",
    "social_category_to_group={'ostwest': ['Westdeutschland', 'Ostdeutschland'],\n",
    " 'berufabschluss_clause': ['hat einen Berufsfachschulabschluss.',\n",
    "  'hat einen Fachhochschulabschluss.',\n",
    "  'hat einen Universit√§tsabschluss.',\n",
    "  'hat eine kaufm√§nnische Lehre abgeschlossen.',\n",
    "  'hat einen Meisterabschluss oder Technikerabschluss.',\n",
    "  'hat eine Lehre abgeschlossen.',\n",
    "  'hat keine berufliche Ausbildung abgeschlossen.',\n",
    "  'hat einen Fachschulabschluss.',\n",
    "  'befindet sich noch in beruflicher Ausbildung.',\n",
    "  'hat ein Berufliches Praktikum oder Volontariat abgeschlossen.',\n",
    "  'hat eine gewerbliche oder landwirtschaftliche Lehre abgeschlossen.'],\n",
    " 'leaning_party': ['die Gr√ºnen',\n",
    "  'Die Linke',\n",
    "  'die CDU/CSU',\n",
    "  'die FDP',\n",
    "  'die SPD',\n",
    "  'die AfD',\n",
    "  'keine Partei',\n",
    "  'eine Kleinpartei'],\n",
    " 'gender': ['weiblich', 'm√§nnlich'],\n",
    " 'schulabschluss_clause': ['hat einen Fachhochschulreife',\n",
    "  'hat das Abitur',\n",
    "  'hat einen Realschulabschluss',\n",
    "  'hat einen Hauptschulabschluss',\n",
    "  'hat keinen Schulabschluss',\n",
    "  'ist noch Sch√ºler/in'],\n",
    " 'age_groups': ['45-59 YEARS', '60 and more', '30-44 YEARS', '18-29 YEARS']}\n",
    "social_group_to_category = {v: k for k, vals in social_category_to_group.items() for v in vals}\n",
    "wave_dates={\n",
    "10: '06-11-2018',\n",
    "11: '28-05-2019',\n",
    "12: '05-11-2019',\n",
    " 13: '21-04-2020',\n",
    " 14: '03-11-2020',\n",
    " 15: '25-02-2021',\n",
    " 16: '06-05-2021',\n",
    " 17: '07-07-2021',\n",
    " 18: '11-08-2021',\n",
    " 19: '15-09-2021',\n",
    " 20: '29-09-2021',\n",
    " 21: '09-12-2021'}\n",
    "\n",
    "# from src.analysis.waveExperiment.utils import get_waveExperiment_data \n",
    "# import time \n",
    "# begin=time.time()\n",
    "# (\n",
    "#         survey_labels_dict,\n",
    "#         llm_labels_dict,\n",
    "#         # multilabel\n",
    "#         survey_population_df_multilabel,  # df\n",
    "#         llm_population_df_multilabel,  # df\n",
    "#         survey_group_pmf_multilabel,  # dict of dfs\n",
    "#         llm_group_pmf_multilabel,  # dict of dfs\n",
    "#         # multiclass\n",
    "#         survey_population_df_multiclass,  # df\n",
    "#         llm_population_df_multiclass,  # df\n",
    "#         survey_group_pmf_multiclass,  # dict of dfs\n",
    "#         llm_group_pmf_multiclass,  # dict of dfs\n",
    "#     ) = get_waveExperiment_data(until=16)\n",
    "# end=time.time()\n",
    "# print(end-begin)\n",
    "\n",
    "# MI_results_waveExperiment= get_MI_experiment(survey_labels_dict,llm_labels_dict)\n",
    "# population_JS,group_JS= get_JS_waveExperiment(survey_population_df,llm_population_df,survey_group_pmf,llm_group_pmf)\n",
    "# population_level_entropy_results,group_level_entropy_results = get_waveExperiment_Entropy(survey_population_df,llm_population_df,survey_group_pmf,llm_group_pmf)\n",
    "# cramer_results=get_cramerV_waveExperiment(survey_labels_dict,llm_labels_dict)\n",
    "# cramer_results2=get_cramerV_waveExperiment2(survey_labels_dict,llm_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
